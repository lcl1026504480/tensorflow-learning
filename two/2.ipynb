{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T05:35:39.253957Z",
     "start_time": "2019-10-06T05:35:35.349390Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 [[0.46632278 0.5817855 ]] [-0.2640162]\n",
      "20 [[0.22634637 0.3584133 ]] [0.14649247]\n",
      "40 [[0.13998543 0.25508216]] [0.24867949]\n",
      "60 [[0.11302076 0.21881896]] [0.28279975]\n",
      "80 [[0.10430538 0.2063754 ]] [0.29422805]\n",
      "100 [[0.10143489 0.20215088]] [0.29806188]\n",
      "120 [[0.10048017 0.20072417]] [0.29934898]\n",
      "140 [[0.10016101 0.20024356]] [0.2997813]\n",
      "160 [[0.10005405 0.20008187]] [0.29992652]\n",
      "180 [[0.10001816 0.20002751]] [0.2999753]\n",
      "200 [[0.10000609 0.20000924]] [0.29999173]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 使用 NumPy 生成假数据(phony data), 总共 100 个点.\n",
    "x_data = np.float32(np.random.rand(2, 100)) # 随机输入\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "# 构造一个线性模型\n",
    "# \n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "# 最小化方差\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 启动图 (graph)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 拟合平面\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print (step, sess.run(W), sess.run(b))\n",
    "\n",
    "# 得到最佳拟合结果 W: [[0.100  0.200]], b: [0.300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T13:42:49.401202Z",
     "start_time": "2019-10-03T13:42:49.392220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_5:0' shape=(1, 1) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点\n",
    "# 加到默认图中.\n",
    "#\n",
    "# 构造器的返回值代表该常量 op 的返回值.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# 创建另外一个常量 op, 产生一个 2x1 矩阵.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.\n",
    "# 返回值 'product' 代表矩阵乘法的结果.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T13:43:34.618758Z",
     "start_time": "2019-10-03T13:43:34.151308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 启动默认图.\n",
    "sess = tf.Session()\n",
    "\n",
    "# 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. \n",
    "# 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回\n",
    "# 矩阵乘法 op 的输出.\n",
    "#\n",
    "# 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.\n",
    "# \n",
    "# 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.\n",
    "#\n",
    "# 返回值 'result' 是一个 numpy `ndarray` 对象.\n",
    "result = sess.run(product)\n",
    "print (result)\n",
    "# ==> [[ 12.]]\n",
    "\n",
    "# 任务完成, 关闭会话.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T13:49:25.862661Z",
     "start_time": "2019-10-03T13:49:25.503351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  result = sess.run([product])\n",
    "  print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T13:50:56.871055Z",
     "start_time": "2019-10-03T13:50:56.511152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  result = sess.run(product)\n",
    "  print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:03:03.135412Z",
     "start_time": "2019-10-03T14:03:02.768271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# 进入一个交互式 TensorFlow 会话.\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# 使用初始化器 initializer op 的 run() 方法初始化 'x' \n",
    "x.initializer.run()\n",
    "\n",
    "# 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 \n",
    "sub = tf.subtract(x, a)\n",
    "print (sub.eval())\n",
    "# ==> [-2. -1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T09:12:41.513133Z",
     "start_time": "2019-10-04T09:12:36.801475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 创建一个变量, 初始化为标量 0.\n",
    "import tensorflow as tf\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# 创建一个 op, 其作用是使 state 增加 1\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# 启动图后, 变量必须先经过`初始化` (init) op 初始化,\n",
    "# 首先必须增加一个`初始化` op 到图中.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# 启动图, 运行 op\n",
    "with tf.Session() as sess:\n",
    "  # 运行 'init' op\n",
    "  sess.run(init_op)\n",
    "  # 打印 'state' 的初始值\n",
    "  print (sess.run(state))\n",
    "  # 运行 op, 更新 'state', 并打印 'state'\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print (sess.run(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T09:54:30.444777Z",
     "start_time": "2019-10-04T09:54:30.046536Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n",
      "<tf.Variable 'counter_1:0' shape=() dtype=int32_ref>\n",
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n",
      "<tf.Variable 'counter_1:0' shape=() dtype=int32_ref>\n",
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n",
      "<tf.Variable 'counter_1:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# 创建一个 op, 其作用是使 state 增加 1\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# 启动图后, 变量必须先经过`初始化` (init) op 初始化,\n",
    "# 首先必须增加一个`初始化` op 到图中.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# 启动图, 运行 op\n",
    "with tf.Session() as sess:\n",
    "  # 运行 'init' op\n",
    "  sess.run(init_op)\n",
    "  # 打印 'state' 的初始值\n",
    "  print (sess.run(state))\n",
    "  # 运行 op, 更新 'state', 并打印 'state'\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print (new_value)\n",
    "    print (state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T10:08:51.926794Z",
     "start_time": "2019-10-04T10:08:51.516498Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input1 = tf.constant(3.0)\n",
    "input2 = tf.constant(2.0)\n",
    "input3 = tf.constant(5.0)\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = tf.multiply(input1, intermed)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print (result)\n",
    "\n",
    "# 输出:\n",
    "# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:26:26.268263Z",
     "start_time": "2019-10-04T12:26:25.835466Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([14.], dtype=float32)]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_19' with dtype float\n\t [[node Placeholder_19 (defined at <ipython-input-24-24f18008bd1c>:3) ]]\n\nOriginal stack trace for 'Placeholder_19':\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"D:\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-24f18008bd1c>\", line 3, in <module>\n    input2 = tf.placeholder(tf.float32)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7401, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_19' with dtype float\n\t [[{{node Placeholder_19}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-24f18008bd1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m7.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_19' with dtype float\n\t [[node Placeholder_19 (defined at <ipython-input-24-24f18008bd1c>:3) ]]\n\nOriginal stack trace for 'Placeholder_19':\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"D:\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-24f18008bd1c>\", line 3, in <module>\n    input2 = tf.placeholder(tf.float32)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7401, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output1 = tf.multiply(input1, input2)\n",
    "output2 = tf.add(input1, input2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output1], feed_dict={input1: [7.], input2: [2.]}))\n",
    "    print(sess.run([output2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T10:28:32.368933Z",
     "start_time": "2019-10-04T10:28:31.976685Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([14.], dtype=float32)]\n",
      "[array([9.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output1 = tf.multiply(input1, input2)\n",
    "output2 = tf.add(input1, input2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output1], feed_dict={input1: [7.], input2: [2.]}))\n",
    "    print(sess.run([output2],feed_dict={input1: [7.], input2: [2.]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T10:33:03.174562Z",
     "start_time": "2019-10-04T10:33:02.244297Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b2d92ae9138e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m7.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'a'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output1 = tf.multiply(input1, input2)\n",
    "output2 = tf.add(input1, input2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output1], feed_dict={input1: ['a'], input2: [2.]}))\n",
    "    print(sess.run([output2],feed_dict={input1: [7.], input2: [2.]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:36:03.020456Z",
     "start_time": "2019-10-04T12:36:00.914892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.9055\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run([train_step,W], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T14:53:44.337388Z",
     "start_time": "2019-10-04T14:50:12.138236Z"
    }
   },
   "source": [
    "# CNN实现MNIST识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T14:53:44.337388Z",
     "start_time": "2019-10-04T14:50:12.138236Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print (\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print (\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T05:40:53.308074Z",
     "start_time": "2019-10-05T05:40:53.297073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 1 1 0 1]\n",
      "[ 1.  0.  0.  4.  5.  0.  7.  8.  0. 10.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.6666666,  0.       ,  0.       ,  6.6666665,  8.333333 ,\n",
       "        0.       , 11.666666 , 13.333333 ,  0.       , 16.666666 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "# dropout函数的实现\n",
    "def dropout(x, level):\n",
    "    if level < 0. or level >= 1: #level是概率值，必须在0~1之间\n",
    "        raise ValueError('Dropout level must be in interval [0, 1[.')\n",
    "    retain_prob = 1. - level\n",
    " \n",
    "    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样\n",
    "    # 硬币 正面的概率为p，n表示每个神经元试验的次数\n",
    "    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。\n",
    "    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了\n",
    "    print(random_tensor)\n",
    " \n",
    "    x *= random_tensor\n",
    "    print(x)\n",
    "    x /= retain_prob\n",
    " \n",
    "    return x\n",
    " \n",
    "#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  \n",
    "x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)\n",
    "dropout(x,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运作方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T11:26:49.227863Z",
     "start_time": "2019-10-05T11:26:49.083250Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-da189c28b7a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    150\u001b[0m  )\n\u001b[0;32m    151\u001b[0m  \u001b[0mFLAGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m  \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m       \u001b[0m_run_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36m_run_main\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T11:25:26.277382Z",
     "start_time": "2019-10-05T11:25:18.801478Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.31 (0.186 sec)\n",
      "Step 100: loss = 2.15 (0.002 sec)\n",
      "Step 200: loss = 1.93 (0.001 sec)\n",
      "Step 300: loss = 1.52 (0.002 sec)\n",
      "Step 400: loss = 1.26 (0.001 sec)\n",
      "Step 500: loss = 1.04 (0.002 sec)\n",
      "Step 600: loss = 0.66 (0.002 sec)\n",
      "Step 700: loss = 0.71 (0.001 sec)\n",
      "Step 800: loss = 0.62 (0.001 sec)\n",
      "Step 900: loss = 0.66 (0.001 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 47602  Precision @ 1: 0.8655\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4384  Precision @ 1: 0.8768\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 8736  Precision @ 1: 0.8736\n",
      "Step 1000: loss = 0.48 (0.017 sec)\n",
      "Step 1100: loss = 0.40 (0.123 sec)\n",
      "Step 1200: loss = 0.44 (0.002 sec)\n",
      "Step 1300: loss = 0.37 (0.001 sec)\n",
      "Step 1400: loss = 0.43 (0.001 sec)\n",
      "Step 1500: loss = 0.38 (0.001 sec)\n",
      "Step 1600: loss = 0.51 (0.002 sec)\n",
      "Step 1700: loss = 0.35 (0.001 sec)\n",
      "Step 1800: loss = 0.26 (0.001 sec)\n",
      "Step 1900: loss = 0.37 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 49395  Precision @ 1: 0.8981\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4518  Precision @ 1: 0.9036\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9022  Precision @ 1: 0.9022\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "FLAGS = None\n",
    "def placeholder_inputs(batch_size):\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         mnist.IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,FLAGS.fake_data)\n",
    "  feed_dict = {images_pl: images_feed,labels_pl: labels_feed}\n",
    "  return feed_dict\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "  num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "def run_training():\n",
    "  data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "  with tf.Graph().as_default():\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "    logits = mnist.inference(images_placeholder,FLAGS.hidden1,FLAGS.hidden2)\n",
    "    loss = mnist.loss(logits, labels_placeholder)\n",
    "    train_op = mnist.training(loss, FLAGS.learning_rate)\n",
    "    eval_correct = mnist.evaluation(logits, labels_placeholder)\n",
    "    summary = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    sess.run(init)\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      feed_dict = fill_feed_dict(data_sets.train,images_placeholder,labels_placeholder)\n",
    "\n",
    "      _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)\n",
    "      duration = time.time() - start_time\n",
    "      if step % 100 == 0:\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.train)\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.validation)\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "  run_training()\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=float,\n",
    "      default=0.01,\n",
    "      help='Initial learning rate.'\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "      '--max_steps',\n",
    "      type=int,\n",
    "      default=2000,\n",
    "      help='Number of steps to run trainer.'\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "      '--hidden1',\n",
    "      type=int,\n",
    "      default=128,\n",
    "      help='Number of units in hidden layer 1.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--hidden2',\n",
    "      type=int,\n",
    "      default=32,\n",
    "      help='Number of units in hidden layer 2.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Batch size.  Must divide evenly into the dataset sizes.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--input_data_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(os.getenv('TEST_TMPDIR', '/MNIST_data')\n",
    "                           ),\n",
    "      help='Directory to put the input data.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default=os.path.join(os.getenv('TEST_TMPDIR', './tmp')),\n",
    "      help='Directory to put the log data.'\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "      '--fake_data',\n",
    "      default=False,\n",
    "      help='If true, uses fake data for unit testing.',\n",
    "      action='store_true'\n",
    "\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-shot编码示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T07:16:56.705496Z",
     "start_time": "2019-10-05T07:16:56.284607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_8:0\", shape=(3,), dtype=int32)\n",
      "This is result1:\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "This is result2:\n",
      "[0. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "This is result3:\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    " \n",
    " \n",
    "BATCHSIZE=6\n",
    "label=tf.expand_dims(tf.constant([0,2,3,6,7,9]),1)\n",
    "index=tf.expand_dims(tf.range(0, BATCHSIZE),1)\n",
    "#use a matrix\n",
    "concated = tf.concat( [index, label],1)\n",
    "onehot_labels = tf.sparse_to_dense(concated, tf.stack([BATCHSIZE,10]), 1.0, 0.0)\n",
    " \n",
    "#use a vector\n",
    "concated2=tf.constant([1,3,4])\n",
    "print(concated2)\n",
    "#onehot_labels2 = tf.sparse_to_dense(concated2, tf.pack([BATCHSIZE,10]), 1.0, 0.0)#cant use ,because output_shape is not a vector\n",
    "onehot_labels2 = tf.sparse_to_dense(concated2, tf.stack([10]), 1.0, 0.0)#can use\n",
    " \n",
    "#use a scalar\n",
    "concated3=tf.constant(5)\n",
    "onehot_labels3 = tf.sparse_to_dense(concated3, tf.stack([10]), 1.0, 0.0)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    result1=sess.run(onehot_labels)\n",
    "    result2 = sess.run(onehot_labels2)\n",
    "    result3 = sess.run(onehot_labels3)\n",
    "    print (\"This is result1:\")\n",
    "    print (result1)\n",
    "    print (\"This is result2:\")\n",
    "    print (result2)\n",
    "    print (\"This is result3:\")\n",
    "    print (result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf输出的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T07:48:20.628420Z",
     "start_time": "2019-10-05T07:48:20.192846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79. 79.]\n",
      "42.0\n",
      "[79.0, 79.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_38_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "t = tf.constant(42.0)\n",
    "u = tf.constant(37.0)\n",
    "tu = tf.add(t, u)\n",
    "ut = tf.add(u, t)\n",
    "with tf.Session() as sess:\n",
    "   print(tf.stack([tu,ut]).eval())  # runs one step\n",
    "#    print(tf.concat([tu,ut],0).eval())这个会报错\n",
    "   print(t.eval()) # runs one step\n",
    "   print(sess.run([tu, ut]) )\n",
    "# evaluates both tensors in a single step\n",
    "a=tf.constant(10)\n",
    "tf.summary.scalar(a.op.name, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-06T06:39:54.377Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_train.py:126: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_train.py:119: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:39:56.936156 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_train.py:119: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_train.py:121: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:39:56.938160 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_train.py:121: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_train.py:62: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:39:56.941142 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_train.py:62: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "I1006 14:39:56.948149 15980 dataset_builder.py:187] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
      "I1006 14:39:59.609820 15980 dataset_info.py:410] Loading info from GCS for cifar10/1.0.2\n",
      "I1006 14:40:06.782118 15980 dataset_builder.py:273] Generating dataset cifar10 (C:\\Users\\lenovouser\\tensorflow_datasets\\cifar10\\1.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cifar10 (162.17 MiB) to C:\\Users\\lenovouser\\tensorflow_datasets\\cifar10\\1.0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d87d9edecd4526b3179ce3cfe23830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aea953adaed4dc69b6830525bec0af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94700f05ff62424d8b8ba0ec3b0ef9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Extraction completed...', max=1, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:40:06.929217 15980 download_manager.py:241] Downloading https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz into C:\\Users\\lenovouser\\tensorflow_datasets\\downloads\\cs.toronto.edu_kriz_cifar-10-binaryODHPtIjLh3oLcXirEISTO7dkzyKjRCuol6lV8Wc6C7s.tar.gz.tmp.3017855ecbe7415296ab04e8f49e8ab3...\n",
      "D:\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:57:51.998779 15980 dataset_builder.py:812] Generating split train\n",
      "I1006 14:57:52.018719 15980 file_format_adapter.py:233] Writing TFRecords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=10, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:36.535984 15980 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:42.459875 15980 dataset_builder.py:812] Generating split test\n",
      "I1006 14:58:42.464861 15980 file_format_adapter.py:233] Writing TFRecords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=1, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=10000, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:52.935219 15980 dataset_builder.py:301] Skipping computing stats for mode ComputeStatsMode.AUTO.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar10 downloaded and prepared to C:\\Users\\lenovouser\\tensorflow_datasets\\cifar10\\1.0.2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:53.288691 15980 dataset_builder.py:399] Constructing tf.data.Dataset for split train, from C:\\Users\\lenovouser\\tensorflow_datasets\\cifar10\\1.0.2\n",
      "W1006 14:58:53.293677 15980 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_input.py:64: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.060832 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_input.py:64: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.107681 15980 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_input.py:45: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.123666 15980 deprecation.py:323] From D:\\jupyter\\cifar10_input.py:45: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_input.py:48: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.272500 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_input.py:48: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.277487 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.280481 15980 deprecation.py:506] From D:\\jupyter\\cifar10.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:102: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.281477 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:102: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:85: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.377017 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:85: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:86: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.380007 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:86: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:190: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.398995 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:190: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.448824 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:270: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.542573 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:270: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:318: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.545564 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:318: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:54.589455 15980 summary_op_util.py:66] Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:54.595432 15980 summary_op_util.py:66] Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:54.600458 15980 summary_op_util.py:66] Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:54.606404 15980 summary_op_util.py:66] Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:330: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.611416 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:330: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:54.742040 15980 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_train.py:81: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:55.057541 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_train.py:81: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_train.py:107: The name tf.train.MonitoredTrainingSession is deprecated. Please use tf.compat.v1.train.MonitoredTrainingSession instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:55.059524 15980 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_train.py:107: The name tf.train.MonitoredTrainingSession is deprecated. Please use tf.compat.v1.train.MonitoredTrainingSession instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:55.061519 15980 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 14:58:55.507066 15980 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:55.598821 15980 monitored_session.py:240] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:57.864461 15980 session_manager.py:500] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:57.886416 15980 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 14:58:58.609275 15980 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 14:59:16.306925: step 0, loss = 4.68 (61.3 examples/sec; 2.090 sec/batch)\n",
      "2019-10-06 14:59:34.670774: step 10, loss = 4.61 (69.7 examples/sec; 1.836 sec/batch)\n",
      "2019-10-06 14:59:52.477361: step 20, loss = 4.46 (71.9 examples/sec; 1.781 sec/batch)\n",
      "2019-10-06 15:00:10.767865: step 30, loss = 4.40 (70.0 examples/sec; 1.829 sec/batch)\n",
      "2019-10-06 15:00:30.053823: step 40, loss = 4.30 (66.4 examples/sec; 1.929 sec/batch)\n",
      "2019-10-06 15:00:50.501135: step 50, loss = 4.22 (62.6 examples/sec; 2.045 sec/batch)\n",
      "2019-10-06 15:01:12.492399: step 60, loss = 4.39 (58.2 examples/sec; 2.199 sec/batch)\n",
      "2019-10-06 15:01:36.261143: step 70, loss = 4.15 (53.9 examples/sec; 2.377 sec/batch)\n",
      "2019-10-06 15:02:01.209464: step 80, loss = 4.27 (51.3 examples/sec; 2.495 sec/batch)\n",
      "2019-10-06 15:02:27.091604: step 90, loss = 4.00 (49.5 examples/sec; 2.588 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.457149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:02:55.066857 15980 basic_session_run_hooks.py:692] global_step/sec: 0.457149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:02:55.096780: step 100, loss = 4.03 (45.7 examples/sec; 2.801 sec/batch)\n",
      "2019-10-06 15:03:18.656281: step 110, loss = 4.05 (54.3 examples/sec; 2.356 sec/batch)\n",
      "2019-10-06 15:03:43.213893: step 120, loss = 4.18 (52.1 examples/sec; 2.456 sec/batch)\n",
      "2019-10-06 15:04:07.344868: step 130, loss = 4.02 (53.0 examples/sec; 2.413 sec/batch)\n",
      "2019-10-06 15:04:31.521304: step 140, loss = 4.16 (52.9 examples/sec; 2.418 sec/batch)\n",
      "2019-10-06 15:04:55.360543: step 150, loss = 3.96 (53.7 examples/sec; 2.384 sec/batch)\n",
      "2019-10-06 15:05:18.252144: step 160, loss = 3.95 (55.9 examples/sec; 2.289 sec/batch)\n",
      "2019-10-06 15:05:41.897389: step 170, loss = 3.94 (54.1 examples/sec; 2.365 sec/batch)\n",
      "2019-10-06 15:06:04.678740: step 180, loss = 3.88 (56.2 examples/sec; 2.278 sec/batch)\n",
      "2019-10-06 15:06:27.376883: step 190, loss = 3.82 (56.4 examples/sec; 2.270 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.427055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:06:49.200746 15980 basic_session_run_hooks.py:692] global_step/sec: 0.427055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:06:49.227675: step 200, loss = 3.89 (58.6 examples/sec; 2.185 sec/batch)\n",
      "2019-10-06 15:07:12.797003: step 210, loss = 3.85 (54.3 examples/sec; 2.357 sec/batch)\n",
      "2019-10-06 15:07:36.006925: step 220, loss = 3.79 (55.2 examples/sec; 2.321 sec/batch)\n",
      "2019-10-06 15:07:59.919171: step 230, loss = 3.65 (53.5 examples/sec; 2.391 sec/batch)\n",
      "2019-10-06 15:08:24.654152: step 240, loss = 3.91 (51.7 examples/sec; 2.473 sec/batch)\n",
      "2019-10-06 15:08:48.353752: step 250, loss = 3.55 (54.0 examples/sec; 2.370 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 256 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:09:00.466986 15980 basic_session_run_hooks.py:606] Saving checkpoints for 256 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:09:12.386637: step 260, loss = 3.59 (53.3 examples/sec; 2.403 sec/batch)\n",
      "2019-10-06 15:09:36.332591: step 270, loss = 3.66 (53.5 examples/sec; 2.395 sec/batch)\n",
      "2019-10-06 15:10:00.019323: step 280, loss = 3.65 (54.0 examples/sec; 2.369 sec/batch)\n",
      "2019-10-06 15:10:22.879904: step 290, loss = 3.56 (56.0 examples/sec; 2.286 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.422915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:10:45.654993 15980 basic_session_run_hooks.py:692] global_step/sec: 0.422915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:10:45.654993: step 300, loss = 3.63 (56.2 examples/sec; 2.278 sec/batch)\n",
      "2019-10-06 15:11:09.175395: step 310, loss = 3.73 (54.4 examples/sec; 2.352 sec/batch)\n",
      "2019-10-06 15:11:32.790818: step 320, loss = 3.47 (54.2 examples/sec; 2.362 sec/batch)\n",
      "2019-10-06 15:11:56.352777: step 330, loss = 3.48 (54.3 examples/sec; 2.356 sec/batch)\n",
      "2019-10-06 15:12:19.277001: step 340, loss = 3.56 (55.8 examples/sec; 2.292 sec/batch)\n",
      "2019-10-06 15:12:42.206582: step 350, loss = 3.53 (55.8 examples/sec; 2.293 sec/batch)\n",
      "2019-10-06 15:13:05.288847: step 360, loss = 3.46 (55.5 examples/sec; 2.308 sec/batch)\n",
      "2019-10-06 15:13:29.176132: step 370, loss = 3.53 (53.6 examples/sec; 2.389 sec/batch)\n",
      "2019-10-06 15:13:54.683292: step 380, loss = 3.42 (50.2 examples/sec; 2.551 sec/batch)\n",
      "2019-10-06 15:14:20.930771: step 390, loss = 3.46 (48.8 examples/sec; 2.625 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.415034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:14:46.599022 15980 basic_session_run_hooks.py:692] global_step/sec: 0.415034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:14:46.614644: step 400, loss = 3.44 (49.8 examples/sec; 2.568 sec/batch)\n",
      "2019-10-06 15:15:09.889896: step 410, loss = 3.18 (55.0 examples/sec; 2.328 sec/batch)\n",
      "2019-10-06 15:15:34.419534: step 420, loss = 3.39 (52.2 examples/sec; 2.453 sec/batch)\n",
      "2019-10-06 15:15:57.704991: step 430, loss = 3.31 (55.0 examples/sec; 2.329 sec/batch)\n",
      "2019-10-06 15:16:20.527484: step 440, loss = 3.25 (56.1 examples/sec; 2.282 sec/batch)\n",
      "2019-10-06 15:16:43.327503: step 450, loss = 3.37 (56.1 examples/sec; 2.280 sec/batch)\n",
      "2019-10-06 15:17:06.372502: step 460, loss = 3.25 (55.5 examples/sec; 2.304 sec/batch)\n",
      "2019-10-06 15:17:30.117330: step 470, loss = 3.16 (53.9 examples/sec; 2.374 sec/batch)\n",
      "2019-10-06 15:17:54.468202: step 480, loss = 3.32 (52.6 examples/sec; 2.435 sec/batch)\n",
      "2019-10-06 15:18:17.577198: step 490, loss = 3.03 (55.4 examples/sec; 2.311 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.426943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:18:40.822442 15980 basic_session_run_hooks.py:692] global_step/sec: 0.426943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:18:40.845691: step 500, loss = 3.15 (55.0 examples/sec; 2.327 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 510 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:19:01.860486 15980 basic_session_run_hooks.py:606] Saving checkpoints for 510 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:19:04.496896: step 510, loss = 3.11 (54.1 examples/sec; 2.365 sec/batch)\n",
      "2019-10-06 15:19:28.311966: step 520, loss = 3.30 (53.7 examples/sec; 2.382 sec/batch)\n",
      "2019-10-06 15:19:51.579769: step 530, loss = 3.26 (55.0 examples/sec; 2.327 sec/batch)\n",
      "2019-10-06 15:20:14.656859: step 540, loss = 3.01 (55.5 examples/sec; 2.308 sec/batch)\n",
      "2019-10-06 15:20:37.575738: step 550, loss = 3.17 (55.8 examples/sec; 2.292 sec/batch)\n",
      "2019-10-06 15:21:00.842124: step 560, loss = 3.20 (55.0 examples/sec; 2.327 sec/batch)\n",
      "2019-10-06 15:21:23.621214: step 570, loss = 3.12 (56.2 examples/sec; 2.278 sec/batch)\n",
      "2019-10-06 15:21:46.564374: step 580, loss = 3.04 (55.8 examples/sec; 2.294 sec/batch)\n",
      "2019-10-06 15:22:09.829547: step 590, loss = 2.94 (55.0 examples/sec; 2.327 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.430406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:22:33.161321 15980 basic_session_run_hooks.py:692] global_step/sec: 0.430406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:22:33.297619: step 600, loss = 3.01 (54.5 examples/sec; 2.347 sec/batch)\n",
      "2019-10-06 15:22:56.353248: step 610, loss = 3.17 (55.5 examples/sec; 2.306 sec/batch)\n",
      "2019-10-06 15:23:19.664970: step 620, loss = 3.28 (54.9 examples/sec; 2.331 sec/batch)\n",
      "2019-10-06 15:23:42.633006: step 630, loss = 3.09 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 15:24:06.260512: step 640, loss = 3.06 (54.2 examples/sec; 2.363 sec/batch)\n",
      "2019-10-06 15:24:29.369633: step 650, loss = 2.84 (55.4 examples/sec; 2.311 sec/batch)\n",
      "2019-10-06 15:24:52.328850: step 660, loss = 2.93 (55.8 examples/sec; 2.296 sec/batch)\n",
      "2019-10-06 15:25:15.558950: step 670, loss = 2.91 (55.1 examples/sec; 2.323 sec/batch)\n",
      "2019-10-06 15:25:39.250254: step 680, loss = 2.77 (54.0 examples/sec; 2.369 sec/batch)\n",
      "2019-10-06 15:26:02.510047: step 690, loss = 2.75 (55.0 examples/sec; 2.326 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.428696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:26:26.427077 15980 basic_session_run_hooks.py:692] global_step/sec: 0.428696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:26:26.454288: step 700, loss = 2.83 (53.5 examples/sec; 2.394 sec/batch)\n",
      "2019-10-06 15:26:51.040301: step 710, loss = 2.77 (52.1 examples/sec; 2.459 sec/batch)\n",
      "2019-10-06 15:27:15.065044: step 720, loss = 2.79 (53.3 examples/sec; 2.402 sec/batch)\n",
      "2019-10-06 15:27:40.153055: step 730, loss = 2.83 (51.0 examples/sec; 2.509 sec/batch)\n",
      "2019-10-06 15:28:06.470663: step 740, loss = 2.95 (48.6 examples/sec; 2.632 sec/batch)\n",
      "2019-10-06 15:28:30.360876: step 750, loss = 2.74 (53.6 examples/sec; 2.389 sec/batch)\n",
      "2019-10-06 15:28:54.050719: step 760, loss = 2.87 (54.0 examples/sec; 2.369 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 765 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:29:03.861480 15980 basic_session_run_hooks.py:606] Saving checkpoints for 765 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:29:19.241943: step 770, loss = 2.63 (50.8 examples/sec; 2.519 sec/batch)\n",
      "2019-10-06 15:29:42.518476: step 780, loss = 2.91 (55.0 examples/sec; 2.328 sec/batch)\n",
      "2019-10-06 15:30:06.214214: step 790, loss = 2.59 (54.0 examples/sec; 2.370 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.409927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:30:30.372658 15980 basic_session_run_hooks.py:692] global_step/sec: 0.409927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:30:30.376649: step 800, loss = 2.76 (53.0 examples/sec; 2.416 sec/batch)\n",
      "2019-10-06 15:30:54.627785: step 810, loss = 2.62 (52.8 examples/sec; 2.425 sec/batch)\n",
      "2019-10-06 15:31:18.709592: step 820, loss = 3.06 (53.2 examples/sec; 2.408 sec/batch)\n",
      "2019-10-06 15:31:44.391063: step 830, loss = 2.75 (49.8 examples/sec; 2.568 sec/batch)\n",
      "2019-10-06 15:32:09.079218: step 840, loss = 2.62 (51.8 examples/sec; 2.469 sec/batch)\n",
      "2019-10-06 15:32:32.794192: step 850, loss = 2.71 (54.0 examples/sec; 2.371 sec/batch)\n",
      "2019-10-06 15:32:57.692882: step 860, loss = 2.70 (51.4 examples/sec; 2.490 sec/batch)\n",
      "2019-10-06 15:33:21.670751: step 870, loss = 2.71 (53.4 examples/sec; 2.398 sec/batch)\n",
      "2019-10-06 15:33:45.150197: step 880, loss = 2.59 (54.5 examples/sec; 2.348 sec/batch)\n",
      "2019-10-06 15:34:08.588509: step 890, loss = 2.54 (54.6 examples/sec; 2.344 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.413601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:34:32.151606 15980 basic_session_run_hooks.py:692] global_step/sec: 0.413601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:34:32.177921: step 900, loss = 2.46 (54.3 examples/sec; 2.359 sec/batch)\n",
      "2019-10-06 15:34:56.356210: step 910, loss = 2.54 (52.9 examples/sec; 2.418 sec/batch)\n",
      "2019-10-06 15:35:19.941568: step 920, loss = 2.48 (54.3 examples/sec; 2.359 sec/batch)\n",
      "2019-10-06 15:35:43.589325: step 930, loss = 2.62 (54.1 examples/sec; 2.365 sec/batch)\n",
      "2019-10-06 15:36:08.055074: step 940, loss = 2.76 (52.3 examples/sec; 2.447 sec/batch)\n",
      "2019-10-06 15:36:30.829163: step 950, loss = 2.60 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 15:36:54.445116: step 960, loss = 2.99 (54.2 examples/sec; 2.362 sec/batch)\n",
      "2019-10-06 15:37:17.353843: step 970, loss = 2.46 (55.9 examples/sec; 2.291 sec/batch)\n",
      "2019-10-06 15:37:41.460781: step 980, loss = 2.62 (53.1 examples/sec; 2.411 sec/batch)\n",
      "2019-10-06 15:38:04.810558: step 990, loss = 2.33 (54.8 examples/sec; 2.335 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.422875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:38:28.627856 15980 basic_session_run_hooks.py:692] global_step/sec: 0.422875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:38:28.735568: step 1000, loss = 2.56 (53.5 examples/sec; 2.393 sec/batch)\n",
      "2019-10-06 15:38:51.827805: step 1010, loss = 2.32 (55.4 examples/sec; 2.309 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 1017 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:39:05.513202 15980 basic_session_run_hooks.py:606] Saving checkpoints for 1017 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:39:15.273383: step 1020, loss = 2.38 (54.6 examples/sec; 2.345 sec/batch)\n",
      "2019-10-06 15:39:38.368612: step 1030, loss = 2.60 (55.4 examples/sec; 2.310 sec/batch)\n",
      "2019-10-06 15:40:01.343302: step 1040, loss = 2.57 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 15:40:24.212137: step 1050, loss = 2.40 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 15:40:47.144801: step 1060, loss = 2.24 (55.8 examples/sec; 2.293 sec/batch)\n",
      "2019-10-06 15:41:10.159246: step 1070, loss = 2.41 (55.6 examples/sec; 2.301 sec/batch)\n",
      "2019-10-06 15:41:33.065112: step 1080, loss = 2.29 (55.9 examples/sec; 2.291 sec/batch)\n",
      "2019-10-06 15:41:56.146379: step 1090, loss = 2.30 (55.5 examples/sec; 2.308 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.433529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:42:19.292910 15980 basic_session_run_hooks.py:692] global_step/sec: 0.433529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:42:19.410496: step 1100, loss = 2.29 (55.0 examples/sec; 2.326 sec/batch)\n",
      "2019-10-06 15:42:42.717158: step 1110, loss = 2.39 (54.9 examples/sec; 2.331 sec/batch)\n",
      "2019-10-06 15:43:05.818371: step 1120, loss = 2.33 (55.4 examples/sec; 2.310 sec/batch)\n",
      "2019-10-06 15:43:28.575505: step 1130, loss = 2.38 (56.2 examples/sec; 2.276 sec/batch)\n",
      "2019-10-06 15:43:51.571001: step 1140, loss = 2.24 (55.7 examples/sec; 2.300 sec/batch)\n",
      "2019-10-06 15:44:14.600539: step 1150, loss = 2.53 (55.6 examples/sec; 2.303 sec/batch)\n",
      "2019-10-06 15:44:37.592047: step 1160, loss = 2.18 (55.7 examples/sec; 2.299 sec/batch)\n",
      "2019-10-06 15:45:00.786010: step 1170, loss = 2.35 (55.2 examples/sec; 2.319 sec/batch)\n",
      "2019-10-06 15:45:23.814419: step 1180, loss = 2.41 (55.6 examples/sec; 2.303 sec/batch)\n",
      "2019-10-06 15:45:46.812418: step 1190, loss = 2.28 (55.7 examples/sec; 2.300 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.43396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:46:09.729123 15980 basic_session_run_hooks.py:692] global_step/sec: 0.43396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:46:09.830990: step 1200, loss = 2.11 (55.6 examples/sec; 2.302 sec/batch)\n",
      "2019-10-06 15:46:32.713785: step 1210, loss = 2.26 (55.9 examples/sec; 2.288 sec/batch)\n",
      "2019-10-06 15:46:55.470919: step 1220, loss = 2.11 (56.2 examples/sec; 2.276 sec/batch)\n",
      "2019-10-06 15:47:18.889285: step 1230, loss = 2.16 (54.7 examples/sec; 2.342 sec/batch)\n",
      "2019-10-06 15:47:41.897055: step 1240, loss = 2.15 (55.6 examples/sec; 2.301 sec/batch)\n",
      "2019-10-06 15:48:05.367814: step 1250, loss = 2.13 (54.5 examples/sec; 2.347 sec/batch)\n",
      "2019-10-06 15:48:28.527038: step 1260, loss = 2.12 (55.3 examples/sec; 2.316 sec/batch)\n",
      "2019-10-06 15:48:51.323137: step 1270, loss = 2.29 (56.1 examples/sec; 2.280 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 1278 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:49:07.195685 15980 basic_session_run_hooks.py:606] Saving checkpoints for 1278 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 15:49:07.588135 15980 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:49:13.684015: step 1280, loss = 2.09 (57.2 examples/sec; 2.236 sec/batch)\n",
      "2019-10-06 15:49:36.284891: step 1290, loss = 2.00 (56.6 examples/sec; 2.260 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.435419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:49:59.393086 15980 basic_session_run_hooks.py:692] global_step/sec: 0.435419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:49:59.397076: step 1300, loss = 2.23 (55.4 examples/sec; 2.311 sec/batch)\n",
      "2019-10-06 15:50:22.536736: step 1310, loss = 2.09 (55.3 examples/sec; 2.314 sec/batch)\n",
      "2019-10-06 15:50:47.508171: step 1320, loss = 2.14 (51.3 examples/sec; 2.497 sec/batch)\n",
      "2019-10-06 15:51:11.817764: step 1330, loss = 2.07 (52.7 examples/sec; 2.431 sec/batch)\n",
      "2019-10-06 15:51:35.134007: step 1340, loss = 2.66 (54.9 examples/sec; 2.332 sec/batch)\n",
      "2019-10-06 15:51:58.391822: step 1350, loss = 2.14 (55.0 examples/sec; 2.326 sec/batch)\n",
      "2019-10-06 15:52:22.074229: step 1360, loss = 1.99 (54.0 examples/sec; 2.368 sec/batch)\n",
      "2019-10-06 15:52:44.936270: step 1370, loss = 2.04 (56.0 examples/sec; 2.286 sec/batch)\n",
      "2019-10-06 15:53:07.709360: step 1380, loss = 1.99 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 15:53:31.093817: step 1390, loss = 1.97 (54.7 examples/sec; 2.338 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.425576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:53:54.368539 15980 basic_session_run_hooks.py:692] global_step/sec: 0.425576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:53:54.485787: step 1400, loss = 1.97 (54.7 examples/sec; 2.339 sec/batch)\n",
      "2019-10-06 15:54:17.451314: step 1410, loss = 2.05 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 15:54:40.772099: step 1420, loss = 2.24 (54.9 examples/sec; 2.332 sec/batch)\n",
      "2019-10-06 15:55:03.459420: step 1430, loss = 2.18 (56.4 examples/sec; 2.269 sec/batch)\n",
      "2019-10-06 15:55:26.625576: step 1440, loss = 1.99 (55.3 examples/sec; 2.317 sec/batch)\n",
      "2019-10-06 15:55:49.465501: step 1450, loss = 2.16 (56.0 examples/sec; 2.284 sec/batch)\n",
      "2019-10-06 15:56:12.421210: step 1460, loss = 2.02 (55.8 examples/sec; 2.296 sec/batch)\n",
      "2019-10-06 15:56:35.762782: step 1470, loss = 1.83 (54.8 examples/sec; 2.334 sec/batch)\n",
      "2019-10-06 15:56:59.373272: step 1480, loss = 2.14 (54.2 examples/sec; 2.361 sec/batch)\n",
      "2019-10-06 15:57:25.311055: step 1490, loss = 1.91 (49.3 examples/sec; 2.594 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.424198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:57:50.107311 15980 basic_session_run_hooks.py:692] global_step/sec: 0.424198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:57:50.132386: step 1500, loss = 2.29 (51.6 examples/sec; 2.482 sec/batch)\n",
      "2019-10-06 15:58:13.006168: step 1510, loss = 1.91 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 15:58:36.373447: step 1520, loss = 2.11 (54.8 examples/sec; 2.337 sec/batch)\n",
      "2019-10-06 15:58:59.805774: step 1530, loss = 2.10 (54.6 examples/sec; 2.343 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 1535 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 15:59:09.108892 15980 basic_session_run_hooks.py:606] Saving checkpoints for 1535 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 15:59:23.858439: step 1540, loss = 1.91 (53.2 examples/sec; 2.405 sec/batch)\n",
      "2019-10-06 15:59:49.066019: step 1550, loss = 1.96 (50.8 examples/sec; 2.521 sec/batch)\n",
      "2019-10-06 16:00:13.616547: step 1560, loss = 1.78 (52.1 examples/sec; 2.455 sec/batch)\n",
      "2019-10-06 16:00:39.207977: step 1570, loss = 1.93 (50.0 examples/sec; 2.559 sec/batch)\n",
      "2019-10-06 16:01:04.998949: step 1580, loss = 1.81 (49.6 examples/sec; 2.579 sec/batch)\n",
      "2019-10-06 16:01:29.571648: step 1590, loss = 1.75 (52.1 examples/sec; 2.457 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.409429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:01:54.349692 15980 basic_session_run_hooks.py:692] global_step/sec: 0.409429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:01:54.932883: step 1600, loss = 1.76 (50.5 examples/sec; 2.536 sec/batch)\n",
      "2019-10-06 16:02:19.123223: step 1610, loss = 1.85 (52.9 examples/sec; 2.419 sec/batch)\n",
      "2019-10-06 16:02:43.770613: step 1620, loss = 1.99 (51.9 examples/sec; 2.465 sec/batch)\n",
      "2019-10-06 16:03:07.551737: step 1630, loss = 1.95 (53.8 examples/sec; 2.378 sec/batch)\n",
      "2019-10-06 16:03:31.361664: step 1640, loss = 1.96 (53.8 examples/sec; 2.381 sec/batch)\n",
      "2019-10-06 16:03:55.351818: step 1650, loss = 1.85 (53.4 examples/sec; 2.399 sec/batch)\n",
      "2019-10-06 16:04:19.697036: step 1660, loss = 2.05 (52.6 examples/sec; 2.435 sec/batch)\n",
      "2019-10-06 16:04:45.216255: step 1670, loss = 2.00 (50.2 examples/sec; 2.552 sec/batch)\n",
      "2019-10-06 16:05:10.209166: step 1680, loss = 2.05 (51.2 examples/sec; 2.499 sec/batch)\n",
      "2019-10-06 16:05:34.419543: step 1690, loss = 1.78 (52.9 examples/sec; 2.421 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.410224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:05:58.119071 15980 basic_session_run_hooks.py:692] global_step/sec: 0.410224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:05:58.141352: step 1700, loss = 2.06 (54.0 examples/sec; 2.372 sec/batch)\n",
      "2019-10-06 16:06:22.203114: step 1710, loss = 1.83 (53.2 examples/sec; 2.406 sec/batch)\n",
      "2019-10-06 16:06:45.510775: step 1720, loss = 2.10 (54.9 examples/sec; 2.331 sec/batch)\n",
      "2019-10-06 16:07:08.419503: step 1730, loss = 1.77 (55.9 examples/sec; 2.291 sec/batch)\n",
      "2019-10-06 16:07:31.360146: step 1740, loss = 1.86 (55.8 examples/sec; 2.294 sec/batch)\n",
      "2019-10-06 16:07:54.293807: step 1750, loss = 1.80 (55.8 examples/sec; 2.293 sec/batch)\n",
      "2019-10-06 16:08:17.121751: step 1760, loss = 1.81 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 16:08:39.990586: step 1770, loss = 2.09 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 16:09:02.976109: step 1780, loss = 1.81 (55.7 examples/sec; 2.299 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 1784 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:09:09.831773 15980 basic_session_run_hooks.py:606] Saving checkpoints for 1784 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:09:25.559439: step 1790, loss = 1.68 (56.7 examples/sec; 2.258 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.434024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:09:48.522018 15980 basic_session_run_hooks.py:692] global_step/sec: 0.434024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:09:48.525011: step 1800, loss = 1.87 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 16:10:11.563181: step 1810, loss = 1.88 (55.6 examples/sec; 2.304 sec/batch)\n",
      "2019-10-06 16:10:34.937664: step 1820, loss = 1.69 (54.8 examples/sec; 2.337 sec/batch)\n",
      "2019-10-06 16:10:57.943052: step 1830, loss = 1.87 (55.6 examples/sec; 2.301 sec/batch)\n",
      "2019-10-06 16:11:20.885689: step 1840, loss = 1.97 (55.8 examples/sec; 2.294 sec/batch)\n",
      "2019-10-06 16:11:43.743747: step 1850, loss = 1.66 (56.0 examples/sec; 2.286 sec/batch)\n",
      "2019-10-06 16:12:06.674421: step 1860, loss = 1.59 (55.8 examples/sec; 2.293 sec/batch)\n",
      "2019-10-06 16:12:30.462792: step 1870, loss = 1.68 (53.8 examples/sec; 2.379 sec/batch)\n",
      "2019-10-06 16:12:55.029727: step 1880, loss = 1.64 (52.1 examples/sec; 2.457 sec/batch)\n",
      "2019-10-06 16:13:19.120295: step 1890, loss = 1.77 (53.1 examples/sec; 2.409 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.426168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:13:43.170014 15980 basic_session_run_hooks.py:692] global_step/sec: 0.426168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:13:43.284979: step 1900, loss = 1.76 (53.0 examples/sec; 2.416 sec/batch)\n",
      "2019-10-06 16:14:08.327997: step 1910, loss = 1.83 (51.1 examples/sec; 2.504 sec/batch)\n",
      "2019-10-06 16:14:31.870426: step 1920, loss = 1.69 (54.4 examples/sec; 2.354 sec/batch)\n",
      "2019-10-06 16:14:55.835726: step 1930, loss = 1.56 (53.4 examples/sec; 2.397 sec/batch)\n",
      "2019-10-06 16:15:20.399915: step 1940, loss = 1.79 (52.1 examples/sec; 2.456 sec/batch)\n",
      "2019-10-06 16:15:45.306945: step 1950, loss = 1.84 (51.4 examples/sec; 2.491 sec/batch)\n",
      "2019-10-06 16:16:08.779854: step 1960, loss = 1.72 (54.5 examples/sec; 2.347 sec/batch)\n",
      "2019-10-06 16:16:31.266941: step 1970, loss = 1.91 (56.9 examples/sec; 2.249 sec/batch)\n",
      "2019-10-06 16:16:54.000820: step 1980, loss = 1.85 (56.3 examples/sec; 2.273 sec/batch)\n",
      "2019-10-06 16:17:17.643411: step 1990, loss = 1.68 (54.1 examples/sec; 2.364 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.418837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:17:41.926464 15980 basic_session_run_hooks.py:692] global_step/sec: 0.418837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:17:41.947451: step 2000, loss = 1.50 (52.7 examples/sec; 2.430 sec/batch)\n",
      "2019-10-06 16:18:06.243468: step 2010, loss = 1.56 (52.7 examples/sec; 2.430 sec/batch)\n",
      "2019-10-06 16:18:30.686827: step 2020, loss = 1.64 (52.4 examples/sec; 2.444 sec/batch)\n",
      "2019-10-06 16:18:54.377463: step 2030, loss = 1.79 (54.0 examples/sec; 2.369 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 2038 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:19:11.590428 15980 basic_session_run_hooks.py:606] Saving checkpoints for 2038 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:19:19.588762: step 2040, loss = 1.52 (50.8 examples/sec; 2.521 sec/batch)\n",
      "2019-10-06 16:19:43.688306: step 2050, loss = 1.76 (53.1 examples/sec; 2.410 sec/batch)\n",
      "2019-10-06 16:20:08.761443: step 2060, loss = 1.75 (51.1 examples/sec; 2.507 sec/batch)\n",
      "2019-10-06 16:20:32.873150: step 2070, loss = 1.41 (53.1 examples/sec; 2.411 sec/batch)\n",
      "2019-10-06 16:20:57.278871: step 2080, loss = 1.58 (52.4 examples/sec; 2.441 sec/batch)\n",
      "2019-10-06 16:21:22.246346: step 2090, loss = 1.53 (51.3 examples/sec; 2.497 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.408137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:21:46.942295 15980 basic_session_run_hooks.py:692] global_step/sec: 0.408137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:21:47.058498: step 2100, loss = 1.56 (51.6 examples/sec; 2.481 sec/batch)\n",
      "2019-10-06 16:22:12.151376: step 2110, loss = 1.61 (51.0 examples/sec; 2.509 sec/batch)\n",
      "2019-10-06 16:22:36.921456: step 2120, loss = 1.68 (51.7 examples/sec; 2.477 sec/batch)\n",
      "2019-10-06 16:23:02.054750: step 2130, loss = 1.52 (50.9 examples/sec; 2.513 sec/batch)\n",
      "2019-10-06 16:23:25.180008: step 2140, loss = 1.59 (55.4 examples/sec; 2.313 sec/batch)\n",
      "2019-10-06 16:23:47.834417: step 2150, loss = 1.82 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 16:24:10.544675: step 2160, loss = 1.58 (56.4 examples/sec; 2.271 sec/batch)\n",
      "2019-10-06 16:24:34.096684: step 2170, loss = 1.45 (54.3 examples/sec; 2.355 sec/batch)\n",
      "2019-10-06 16:24:57.495633: step 2180, loss = 1.67 (54.7 examples/sec; 2.340 sec/batch)\n",
      "2019-10-06 16:25:20.473300: step 2190, loss = 1.64 (55.7 examples/sec; 2.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.420768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:25:44.602765 15980 basic_session_run_hooks.py:692] global_step/sec: 0.420768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:25:44.713467: step 2200, loss = 1.68 (52.8 examples/sec; 2.424 sec/batch)\n",
      "2019-10-06 16:26:09.491196: step 2210, loss = 1.58 (51.7 examples/sec; 2.478 sec/batch)\n",
      "2019-10-06 16:26:34.156764: step 2220, loss = 1.55 (51.9 examples/sec; 2.467 sec/batch)\n",
      "2019-10-06 16:26:58.258808: step 2230, loss = 1.62 (53.1 examples/sec; 2.410 sec/batch)\n",
      "2019-10-06 16:27:22.380581: step 2240, loss = 1.48 (53.1 examples/sec; 2.412 sec/batch)\n",
      "2019-10-06 16:27:47.075537: step 2250, loss = 1.65 (51.8 examples/sec; 2.469 sec/batch)\n",
      "2019-10-06 16:28:11.142163: step 2260, loss = 1.66 (53.2 examples/sec; 2.407 sec/batch)\n",
      "2019-10-06 16:28:34.137138: step 2270, loss = 1.42 (55.7 examples/sec; 2.299 sec/batch)\n",
      "2019-10-06 16:28:58.683486: step 2280, loss = 1.54 (52.1 examples/sec; 2.455 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 2287 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:29:13.159768 15980 basic_session_run_hooks.py:606] Saving checkpoints for 2287 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:29:22.860335: step 2290, loss = 1.62 (52.9 examples/sec; 2.418 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.412299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:29:47.145382 15980 basic_session_run_hooks.py:692] global_step/sec: 0.412299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:29:47.149372: step 2300, loss = 1.60 (52.7 examples/sec; 2.429 sec/batch)\n",
      "2019-10-06 16:30:12.391579: step 2310, loss = 1.54 (50.7 examples/sec; 2.524 sec/batch)\n",
      "2019-10-06 16:30:35.863800: step 2320, loss = 1.61 (54.5 examples/sec; 2.347 sec/batch)\n",
      "2019-10-06 16:30:58.943072: step 2330, loss = 1.55 (55.5 examples/sec; 2.308 sec/batch)\n",
      "2019-10-06 16:31:23.411973: step 2340, loss = 1.58 (52.3 examples/sec; 2.447 sec/batch)\n",
      "2019-10-06 16:31:48.595617: step 2350, loss = 1.72 (50.8 examples/sec; 2.518 sec/batch)\n",
      "2019-10-06 16:32:14.128261: step 2360, loss = 1.45 (50.1 examples/sec; 2.553 sec/batch)\n",
      "2019-10-06 16:32:39.245891: step 2370, loss = 1.53 (51.0 examples/sec; 2.512 sec/batch)\n",
      "2019-10-06 16:33:04.118367: step 2380, loss = 1.52 (51.5 examples/sec; 2.487 sec/batch)\n",
      "2019-10-06 16:33:28.876147: step 2390, loss = 1.46 (51.7 examples/sec; 2.476 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.406597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:33:53.089386 15980 basic_session_run_hooks.py:692] global_step/sec: 0.406597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:33:53.111462: step 2400, loss = 1.53 (52.8 examples/sec; 2.424 sec/batch)\n",
      "2019-10-06 16:34:17.029453: step 2410, loss = 1.47 (53.5 examples/sec; 2.392 sec/batch)\n",
      "2019-10-06 16:34:41.259647: step 2420, loss = 1.61 (52.8 examples/sec; 2.423 sec/batch)\n",
      "2019-10-06 16:35:06.305659: step 2430, loss = 1.43 (51.1 examples/sec; 2.505 sec/batch)\n",
      "2019-10-06 16:35:31.345688: step 2440, loss = 1.54 (51.1 examples/sec; 2.504 sec/batch)\n",
      "2019-10-06 16:35:56.599145: step 2450, loss = 1.67 (50.7 examples/sec; 2.525 sec/batch)\n",
      "2019-10-06 16:36:21.975273: step 2460, loss = 1.38 (50.4 examples/sec; 2.538 sec/batch)\n",
      "2019-10-06 16:36:46.634319: step 2470, loss = 1.39 (51.9 examples/sec; 2.466 sec/batch)\n",
      "2019-10-06 16:37:10.899420: step 2480, loss = 1.27 (52.8 examples/sec; 2.427 sec/batch)\n",
      "2019-10-06 16:37:35.054452: step 2490, loss = 1.52 (53.0 examples/sec; 2.416 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.404979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:38:00.015740 15980 basic_session_run_hooks.py:692] global_step/sec: 0.404979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:38:00.078344: step 2500, loss = 1.40 (51.2 examples/sec; 2.502 sec/batch)\n",
      "2019-10-06 16:38:24.772701: step 2510, loss = 1.42 (51.8 examples/sec; 2.469 sec/batch)\n",
      "2019-10-06 16:38:48.154163: step 2520, loss = 1.31 (54.7 examples/sec; 2.338 sec/batch)\n",
      "2019-10-06 16:39:11.570534: step 2530, loss = 1.54 (54.7 examples/sec; 2.342 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 2532 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:39:13.946181 15980 basic_session_run_hooks.py:606] Saving checkpoints for 2532 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:39:34.966422: step 2540, loss = 1.34 (54.7 examples/sec; 2.340 sec/batch)\n",
      "2019-10-06 16:39:58.518361: step 2550, loss = 1.56 (54.3 examples/sec; 2.355 sec/batch)\n",
      "2019-10-06 16:40:22.348884: step 2560, loss = 1.50 (53.7 examples/sec; 2.383 sec/batch)\n",
      "2019-10-06 16:40:45.476149: step 2570, loss = 1.35 (55.3 examples/sec; 2.313 sec/batch)\n",
      "2019-10-06 16:41:09.876886: step 2580, loss = 1.61 (52.5 examples/sec; 2.440 sec/batch)\n",
      "2019-10-06 16:41:32.445950: step 2590, loss = 1.37 (56.7 examples/sec; 2.257 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.424935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:41:55.345702 15980 basic_session_run_hooks.py:692] global_step/sec: 0.424935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:41:55.465259: step 2600, loss = 1.35 (55.6 examples/sec; 2.302 sec/batch)\n",
      "2019-10-06 16:42:19.399199: step 2610, loss = 1.54 (53.5 examples/sec; 2.393 sec/batch)\n",
      "2019-10-06 16:42:42.472468: step 2620, loss = 1.48 (55.5 examples/sec; 2.307 sec/batch)\n",
      "2019-10-06 16:43:05.504275: step 2630, loss = 1.33 (55.6 examples/sec; 2.303 sec/batch)\n",
      "2019-10-06 16:43:29.382412: step 2640, loss = 1.38 (53.6 examples/sec; 2.388 sec/batch)\n",
      "2019-10-06 16:43:53.015202: step 2650, loss = 1.69 (54.2 examples/sec; 2.363 sec/batch)\n",
      "2019-10-06 16:44:16.316879: step 2660, loss = 1.52 (54.9 examples/sec; 2.330 sec/batch)\n",
      "2019-10-06 16:44:39.815030: step 2670, loss = 1.42 (54.5 examples/sec; 2.350 sec/batch)\n",
      "2019-10-06 16:45:05.852585: step 2680, loss = 1.30 (49.2 examples/sec; 2.604 sec/batch)\n",
      "2019-10-06 16:45:29.117361: step 2690, loss = 1.40 (55.0 examples/sec; 2.326 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.420327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:45:53.256798 15980 basic_session_run_hooks.py:692] global_step/sec: 0.420327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:45:53.281544: step 2700, loss = 1.24 (53.0 examples/sec; 2.416 sec/batch)\n",
      "2019-10-06 16:46:16.240137: step 2710, loss = 1.30 (55.8 examples/sec; 2.296 sec/batch)\n",
      "2019-10-06 16:46:39.823065: step 2720, loss = 1.42 (54.3 examples/sec; 2.358 sec/batch)\n",
      "2019-10-06 16:47:03.996408: step 2730, loss = 1.71 (53.0 examples/sec; 2.417 sec/batch)\n",
      "2019-10-06 16:47:28.403130: step 2740, loss = 1.35 (52.4 examples/sec; 2.441 sec/batch)\n",
      "2019-10-06 16:47:51.084119: step 2750, loss = 1.32 (56.4 examples/sec; 2.268 sec/batch)\n",
      "2019-10-06 16:48:14.366169: step 2760, loss = 1.32 (55.0 examples/sec; 2.328 sec/batch)\n",
      "2019-10-06 16:48:37.214639: step 2770, loss = 1.53 (56.0 examples/sec; 2.285 sec/batch)\n",
      "2019-10-06 16:49:02.094225: step 2780, loss = 1.46 (51.4 examples/sec; 2.488 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 2786 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:49:14.156928 15980 basic_session_run_hooks.py:606] Saving checkpoints for 2786 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:49:26.479287: step 2790, loss = 1.32 (52.5 examples/sec; 2.439 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.420859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:49:50.865353 15980 basic_session_run_hooks.py:692] global_step/sec: 0.420859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:49:50.868345: step 2800, loss = 1.54 (52.5 examples/sec; 2.439 sec/batch)\n",
      "2019-10-06 16:50:15.258712: step 2810, loss = 1.34 (52.5 examples/sec; 2.439 sec/batch)\n",
      "2019-10-06 16:50:38.739695: step 2820, loss = 1.33 (54.5 examples/sec; 2.348 sec/batch)\n",
      "2019-10-06 16:51:02.635578: step 2830, loss = 1.59 (53.6 examples/sec; 2.390 sec/batch)\n",
      "2019-10-06 16:51:26.669083: step 2840, loss = 1.34 (53.3 examples/sec; 2.403 sec/batch)\n",
      "2019-10-06 16:51:50.727074: step 2850, loss = 1.28 (53.2 examples/sec; 2.406 sec/batch)\n",
      "2019-10-06 16:52:14.772254: step 2860, loss = 1.33 (53.2 examples/sec; 2.405 sec/batch)\n",
      "2019-10-06 16:52:38.587183: step 2870, loss = 1.38 (53.7 examples/sec; 2.381 sec/batch)\n",
      "2019-10-06 16:53:02.069508: step 2880, loss = 1.10 (54.5 examples/sec; 2.348 sec/batch)\n",
      "2019-10-06 16:53:25.765133: step 2890, loss = 1.40 (54.0 examples/sec; 2.370 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.417775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:53:50.229845 15980 basic_session_run_hooks.py:692] global_step/sec: 0.417775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:53:50.253780: step 2900, loss = 1.46 (52.3 examples/sec; 2.449 sec/batch)\n",
      "2019-10-06 16:54:14.478985: step 2910, loss = 1.65 (52.8 examples/sec; 2.423 sec/batch)\n",
      "2019-10-06 16:54:37.943228: step 2920, loss = 1.34 (54.6 examples/sec; 2.346 sec/batch)\n",
      "2019-10-06 16:55:00.549326: step 2930, loss = 1.44 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 16:55:24.758754: step 2940, loss = 1.34 (52.9 examples/sec; 2.421 sec/batch)\n",
      "2019-10-06 16:55:48.136228: step 2950, loss = 1.36 (54.8 examples/sec; 2.338 sec/batch)\n",
      "2019-10-06 16:56:11.035980: step 2960, loss = 1.21 (55.9 examples/sec; 2.290 sec/batch)\n",
      "2019-10-06 16:56:34.688718: step 2970, loss = 1.47 (54.1 examples/sec; 2.365 sec/batch)\n",
      "2019-10-06 16:56:58.020316: step 2980, loss = 1.27 (54.9 examples/sec; 2.333 sec/batch)\n",
      "2019-10-06 16:57:21.302495: step 2990, loss = 1.27 (55.0 examples/sec; 2.328 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.426359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:57:44.772721 15980 basic_session_run_hooks.py:692] global_step/sec: 0.426359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:57:44.800702: step 3000, loss = 1.39 (54.5 examples/sec; 2.350 sec/batch)\n",
      "2019-10-06 16:58:08.212085: step 3010, loss = 1.24 (54.7 examples/sec; 2.341 sec/batch)\n",
      "2019-10-06 16:58:31.786187: step 3020, loss = 1.46 (54.3 examples/sec; 2.357 sec/batch)\n",
      "2019-10-06 16:58:56.364087: step 3030, loss = 1.21 (52.1 examples/sec; 2.458 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 3039 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 16:59:15.420471 15980 basic_session_run_hooks.py:606] Saving checkpoints for 3039 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 16:59:20.396644: step 3040, loss = 1.28 (53.3 examples/sec; 2.403 sec/batch)\n",
      "2019-10-06 16:59:44.129170: step 3050, loss = 1.29 (53.9 examples/sec; 2.373 sec/batch)\n",
      "2019-10-06 17:00:07.857704: step 3060, loss = 1.26 (53.9 examples/sec; 2.373 sec/batch)\n",
      "2019-10-06 17:00:30.723548: step 3070, loss = 1.19 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 17:00:54.444653: step 3080, loss = 1.31 (54.0 examples/sec; 2.372 sec/batch)\n",
      "2019-10-06 17:01:17.446133: step 3090, loss = 1.31 (55.6 examples/sec; 2.300 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.423185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:01:41.075933 15980 basic_session_run_hooks.py:692] global_step/sec: 0.423185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:01:41.101864: step 3100, loss = 1.32 (54.1 examples/sec; 2.366 sec/batch)\n",
      "2019-10-06 17:02:04.393567: step 3110, loss = 1.15 (55.0 examples/sec; 2.329 sec/batch)\n",
      "2019-10-06 17:02:27.297309: step 3120, loss = 1.36 (55.9 examples/sec; 2.290 sec/batch)\n",
      "2019-10-06 17:02:50.513849: step 3130, loss = 1.06 (55.1 examples/sec; 2.322 sec/batch)\n",
      "2019-10-06 17:03:13.341845: step 3140, loss = 1.16 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 17:03:36.199709: step 3150, loss = 1.22 (56.0 examples/sec; 2.286 sec/batch)\n",
      "2019-10-06 17:03:59.166565: step 3160, loss = 1.28 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 17:04:22.141118: step 3170, loss = 1.06 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 17:04:45.059031: step 3180, loss = 1.23 (55.9 examples/sec; 2.292 sec/batch)\n",
      "2019-10-06 17:05:08.508314: step 3190, loss = 1.25 (54.6 examples/sec; 2.345 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.433315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:05:31.854993 15980 basic_session_run_hooks.py:692] global_step/sec: 0.433315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:05:31.880926: step 3200, loss = 1.09 (54.8 examples/sec; 2.337 sec/batch)\n",
      "2019-10-06 17:05:55.402818: step 3210, loss = 1.18 (54.4 examples/sec; 2.352 sec/batch)\n",
      "2019-10-06 17:06:18.750373: step 3220, loss = 1.30 (54.8 examples/sec; 2.335 sec/batch)\n",
      "2019-10-06 17:06:41.863555: step 3230, loss = 1.21 (55.4 examples/sec; 2.311 sec/batch)\n",
      "2019-10-06 17:07:04.787947: step 3240, loss = 1.21 (55.8 examples/sec; 2.292 sec/batch)\n",
      "2019-10-06 17:07:28.007244: step 3250, loss = 1.29 (55.1 examples/sec; 2.322 sec/batch)\n",
      "2019-10-06 17:07:51.426607: step 3260, loss = 1.31 (54.7 examples/sec; 2.342 sec/batch)\n",
      "2019-10-06 17:08:14.297044: step 3270, loss = 1.24 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 17:08:37.812150: step 3280, loss = 1.26 (54.4 examples/sec; 2.352 sec/batch)\n",
      "2019-10-06 17:09:01.299332: step 3290, loss = 1.32 (54.5 examples/sec; 2.349 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 3298 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:09:17.358341 15980 basic_session_run_hooks.py:606] Saving checkpoints for 3298 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.429626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:09:24.616357 15980 basic_session_run_hooks.py:692] global_step/sec: 0.429626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:09:24.619350: step 3300, loss = 1.19 (54.9 examples/sec; 2.332 sec/batch)\n",
      "2019-10-06 17:09:48.020092: step 3310, loss = 1.27 (54.7 examples/sec; 2.340 sec/batch)\n",
      "2019-10-06 17:10:10.481132: step 3320, loss = 1.89 (57.0 examples/sec; 2.246 sec/batch)\n",
      "2019-10-06 17:10:33.245463: step 3330, loss = 1.18 (56.2 examples/sec; 2.276 sec/batch)\n",
      "2019-10-06 17:10:55.894885: step 3340, loss = 1.33 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 17:11:18.500658: step 3350, loss = 1.15 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 17:11:40.992501: step 3360, loss = 1.33 (56.9 examples/sec; 2.249 sec/batch)\n",
      "2019-10-06 17:12:03.440462: step 3370, loss = 1.25 (57.0 examples/sec; 2.245 sec/batch)\n",
      "2019-10-06 17:12:25.881440: step 3380, loss = 1.25 (57.0 examples/sec; 2.244 sec/batch)\n",
      "2019-10-06 17:12:48.321422: step 3390, loss = 1.26 (57.0 examples/sec; 2.244 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.441908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:13:10.906575 15980 basic_session_run_hooks.py:692] global_step/sec: 0.441908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:13:11.024693: step 3400, loss = 1.35 (56.4 examples/sec; 2.270 sec/batch)\n",
      "2019-10-06 17:13:33.625201: step 3410, loss = 1.09 (56.6 examples/sec; 2.260 sec/batch)\n",
      "2019-10-06 17:13:56.586164: step 3420, loss = 1.50 (55.7 examples/sec; 2.296 sec/batch)\n",
      "2019-10-06 17:14:19.157794: step 3430, loss = 1.16 (56.7 examples/sec; 2.257 sec/batch)\n",
      "2019-10-06 17:14:41.603759: step 3440, loss = 1.17 (57.0 examples/sec; 2.245 sec/batch)\n",
      "2019-10-06 17:15:04.143474: step 3450, loss = 1.21 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 17:15:26.608389: step 3460, loss = 1.27 (57.0 examples/sec; 2.246 sec/batch)\n",
      "2019-10-06 17:15:49.370510: step 3470, loss = 1.17 (56.2 examples/sec; 2.276 sec/batch)\n",
      "2019-10-06 17:16:11.978327: step 3480, loss = 1.21 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 17:16:34.511059: step 3490, loss = 1.00 (56.8 examples/sec; 2.253 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.441681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:16:57.314175 15980 basic_session_run_hooks.py:692] global_step/sec: 0.441681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:16:57.396405: step 3500, loss = 1.31 (55.9 examples/sec; 2.289 sec/batch)\n",
      "2019-10-06 17:17:19.893233: step 3510, loss = 1.23 (56.9 examples/sec; 2.250 sec/batch)\n",
      "2019-10-06 17:17:42.500767: step 3520, loss = 1.44 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 17:18:05.129405: step 3530, loss = 1.30 (56.6 examples/sec; 2.263 sec/batch)\n",
      "2019-10-06 17:18:27.581354: step 3540, loss = 1.18 (57.0 examples/sec; 2.245 sec/batch)\n",
      "2019-10-06 17:18:50.122067: step 3550, loss = 1.18 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 17:19:12.573019: step 3560, loss = 1.25 (57.0 examples/sec; 2.245 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 3564 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:19:19.363856 15980 basic_session_run_hooks.py:606] Saving checkpoints for 3564 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:19:35.550978: step 3570, loss = 1.23 (55.7 examples/sec; 2.298 sec/batch)\n",
      "2019-10-06 17:19:58.168485: step 3580, loss = 1.24 (56.6 examples/sec; 2.262 sec/batch)\n",
      "2019-10-06 17:20:20.610462: step 3590, loss = 1.20 (57.0 examples/sec; 2.244 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.442645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:20:43.228967 15980 basic_session_run_hooks.py:692] global_step/sec: 0.442645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:20:43.232956: step 3600, loss = 0.93 (56.6 examples/sec; 2.262 sec/batch)\n",
      "2019-10-06 17:21:05.682205: step 3610, loss = 1.33 (57.0 examples/sec; 2.245 sec/batch)\n",
      "2019-10-06 17:21:28.217121: step 3620, loss = 1.22 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 17:21:50.738869: step 3630, loss = 1.09 (56.8 examples/sec; 2.252 sec/batch)\n",
      "2019-10-06 17:22:13.300525: step 3640, loss = 1.10 (56.7 examples/sec; 2.256 sec/batch)\n",
      "2019-10-06 17:22:35.876289: step 3650, loss = 1.24 (56.7 examples/sec; 2.258 sec/batch)\n",
      "2019-10-06 17:22:58.393454: step 3660, loss = 1.16 (56.8 examples/sec; 2.252 sec/batch)\n",
      "2019-10-06 17:23:20.990189: step 3670, loss = 1.31 (56.6 examples/sec; 2.260 sec/batch)\n",
      "2019-10-06 17:23:43.400732: step 3680, loss = 1.07 (57.1 examples/sec; 2.241 sec/batch)\n",
      "2019-10-06 17:24:05.872533: step 3690, loss = 1.22 (57.0 examples/sec; 2.247 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.443923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:24:28.493512 15980 basic_session_run_hooks.py:692] global_step/sec: 0.443923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:24:28.513973: step 3700, loss = 1.24 (56.5 examples/sec; 2.264 sec/batch)\n",
      "2019-10-06 17:24:50.991853: step 3710, loss = 1.33 (56.9 examples/sec; 2.248 sec/batch)\n",
      "2019-10-06 17:25:13.386954: step 3720, loss = 1.29 (57.2 examples/sec; 2.240 sec/batch)\n",
      "2019-10-06 17:25:35.784052: step 3730, loss = 1.24 (57.2 examples/sec; 2.240 sec/batch)\n",
      "2019-10-06 17:25:58.270804: step 3740, loss = 1.10 (56.9 examples/sec; 2.249 sec/batch)\n",
      "2019-10-06 17:26:20.942167: step 3750, loss = 1.32 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 17:26:43.715961: step 3760, loss = 1.10 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 17:27:06.158934: step 3770, loss = 1.29 (57.0 examples/sec; 2.244 sec/batch)\n",
      "2019-10-06 17:27:28.630600: step 3780, loss = 1.19 (57.0 examples/sec; 2.247 sec/batch)\n",
      "2019-10-06 17:27:51.352828: step 3790, loss = 1.13 (56.3 examples/sec; 2.272 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.443365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:28:14.041146 15980 basic_session_run_hooks.py:692] global_step/sec: 0.443365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:28:14.064095: step 3800, loss = 1.19 (56.4 examples/sec; 2.271 sec/batch)\n",
      "2019-10-06 17:28:36.698554: step 3810, loss = 1.29 (56.6 examples/sec; 2.263 sec/batch)\n",
      "2019-10-06 17:28:59.254893: step 3820, loss = 1.00 (56.7 examples/sec; 2.256 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 3830 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:29:19.529667 15980 basic_session_run_hooks.py:606] Saving checkpoints for 3830 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:29:22.420964: step 3830, loss = 1.32 (55.3 examples/sec; 2.317 sec/batch)\n",
      "2019-10-06 17:29:44.861726: step 3840, loss = 0.98 (57.0 examples/sec; 2.244 sec/batch)\n",
      "2019-10-06 17:30:07.371962: step 3850, loss = 1.00 (56.9 examples/sec; 2.251 sec/batch)\n",
      "2019-10-06 17:30:29.805570: step 3860, loss = 1.20 (57.1 examples/sec; 2.243 sec/batch)\n",
      "2019-10-06 17:30:52.472634: step 3870, loss = 1.16 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 17:31:15.142999: step 3880, loss = 1.20 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 17:31:37.731468: step 3890, loss = 1.13 (56.7 examples/sec; 2.259 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.440618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:32:00.995247 15980 basic_session_run_hooks.py:692] global_step/sec: 0.440618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:32:01.105467: step 3900, loss = 1.40 (54.8 examples/sec; 2.337 sec/batch)\n",
      "2019-10-06 17:32:23.746708: step 3910, loss = 1.15 (56.5 examples/sec; 2.264 sec/batch)\n",
      "2019-10-06 17:32:46.420286: step 3920, loss = 1.12 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 17:33:08.935190: step 3930, loss = 1.04 (56.9 examples/sec; 2.251 sec/batch)\n",
      "2019-10-06 17:33:31.398446: step 3940, loss = 1.17 (57.0 examples/sec; 2.246 sec/batch)\n",
      "2019-10-06 17:33:53.852028: step 3950, loss = 1.23 (57.0 examples/sec; 2.245 sec/batch)\n",
      "2019-10-06 17:34:16.599189: step 3960, loss = 1.19 (56.3 examples/sec; 2.275 sec/batch)\n",
      "2019-10-06 17:34:39.210918: step 3970, loss = 1.24 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 17:35:01.676830: step 3980, loss = 1.13 (57.0 examples/sec; 2.247 sec/batch)\n",
      "2019-10-06 17:35:24.174080: step 3990, loss = 1.29 (56.9 examples/sec; 2.250 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.442676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:35:46.894076 15980 basic_session_run_hooks.py:692] global_step/sec: 0.442676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:35:47.004837: step 4000, loss = 1.24 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 17:36:11.715677: step 4010, loss = 1.17 (51.8 examples/sec; 2.471 sec/batch)\n",
      "2019-10-06 17:36:34.543622: step 4020, loss = 0.98 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 17:36:57.143159: step 4030, loss = 0.93 (56.6 examples/sec; 2.260 sec/batch)\n",
      "2019-10-06 17:37:19.571173: step 4040, loss = 1.05 (57.1 examples/sec; 2.243 sec/batch)\n",
      "2019-10-06 17:37:42.067240: step 4050, loss = 1.16 (56.9 examples/sec; 2.250 sec/batch)\n",
      "2019-10-06 17:38:04.550107: step 4060, loss = 0.95 (56.9 examples/sec; 2.248 sec/batch)\n",
      "2019-10-06 17:38:27.077855: step 4070, loss = 1.16 (56.8 examples/sec; 2.253 sec/batch)\n",
      "2019-10-06 17:38:49.480936: step 4080, loss = 1.38 (57.1 examples/sec; 2.240 sec/batch)\n",
      "2019-10-06 17:39:12.171248: step 4090, loss = 1.01 (56.4 examples/sec; 2.269 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 4095 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:39:21.182674 15980 basic_session_run_hooks.py:606] Saving checkpoints for 4095 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.438002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:39:35.203402 15980 basic_session_run_hooks.py:692] global_step/sec: 0.438002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:39:35.207393: step 4100, loss = 1.02 (55.6 examples/sec; 2.304 sec/batch)\n",
      "2019-10-06 17:39:57.640392: step 4110, loss = 0.93 (57.1 examples/sec; 2.243 sec/batch)\n",
      "2019-10-06 17:40:20.232802: step 4120, loss = 1.00 (56.7 examples/sec; 2.259 sec/batch)\n",
      "2019-10-06 17:40:42.568065: step 4130, loss = 1.00 (57.3 examples/sec; 2.234 sec/batch)\n",
      "2019-10-06 17:41:05.159642: step 4140, loss = 1.24 (56.7 examples/sec; 2.259 sec/batch)\n",
      "2019-10-06 17:41:27.833956: step 4150, loss = 1.09 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 17:41:50.399601: step 4160, loss = 1.09 (56.7 examples/sec; 2.257 sec/batch)\n",
      "2019-10-06 17:42:12.976218: step 4170, loss = 1.08 (56.7 examples/sec; 2.258 sec/batch)\n",
      "2019-10-06 17:42:35.545138: step 4180, loss = 1.09 (56.7 examples/sec; 2.257 sec/batch)\n",
      "2019-10-06 17:42:57.954203: step 4190, loss = 1.21 (57.1 examples/sec; 2.241 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.443661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:43:20.600925 15980 basic_session_run_hooks.py:692] global_step/sec: 0.443661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:43:20.618931: step 4200, loss = 0.98 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 17:43:43.186570: step 4210, loss = 1.14 (56.7 examples/sec; 2.257 sec/batch)\n",
      "2019-10-06 17:44:05.634480: step 4220, loss = 1.05 (57.0 examples/sec; 2.245 sec/batch)\n",
      "2019-10-06 17:44:28.171202: step 4230, loss = 1.37 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 17:44:50.590240: step 4240, loss = 0.83 (57.1 examples/sec; 2.242 sec/batch)\n",
      "2019-10-06 17:45:13.188760: step 4250, loss = 1.25 (56.6 examples/sec; 2.260 sec/batch)\n",
      "2019-10-06 17:45:35.851147: step 4260, loss = 0.94 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 17:45:58.641193: step 4270, loss = 0.99 (56.2 examples/sec; 2.279 sec/batch)\n",
      "2019-10-06 17:46:21.230774: step 4280, loss = 1.18 (56.7 examples/sec; 2.259 sec/batch)\n",
      "2019-10-06 17:46:43.807206: step 4290, loss = 1.03 (56.7 examples/sec; 2.258 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.441002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:47:07.357219 15980 basic_session_run_hooks.py:692] global_step/sec: 0.441002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:47:07.377207: step 4300, loss = 1.21 (54.3 examples/sec; 2.357 sec/batch)\n",
      "2019-10-06 17:47:31.795816: step 4310, loss = 1.24 (52.4 examples/sec; 2.442 sec/batch)\n",
      "2019-10-06 17:47:54.939915: step 4320, loss = 1.10 (55.3 examples/sec; 2.314 sec/batch)\n",
      "2019-10-06 17:48:17.960143: step 4330, loss = 1.15 (55.6 examples/sec; 2.302 sec/batch)\n",
      "2019-10-06 17:48:40.899114: step 4340, loss = 1.17 (55.8 examples/sec; 2.294 sec/batch)\n",
      "2019-10-06 17:49:03.764958: step 4350, loss = 1.04 (56.0 examples/sec; 2.287 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 4359 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:49:22.393134 15980 basic_session_run_hooks.py:606] Saving checkpoints for 4359 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:49:27.841560: step 4360, loss = 1.16 (53.2 examples/sec; 2.408 sec/batch)\n",
      "2019-10-06 17:49:51.683817: step 4370, loss = 1.07 (53.7 examples/sec; 2.384 sec/batch)\n",
      "2019-10-06 17:50:14.415499: step 4380, loss = 1.01 (56.3 examples/sec; 2.273 sec/batch)\n",
      "2019-10-06 17:50:37.267162: step 4390, loss = 1.06 (56.0 examples/sec; 2.285 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.427637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:51:01.200273 15980 basic_session_run_hooks.py:692] global_step/sec: 0.427637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:51:01.204264: step 4400, loss = 1.05 (53.5 examples/sec; 2.394 sec/batch)\n",
      "2019-10-06 17:51:23.680623: step 4410, loss = 0.95 (56.9 examples/sec; 2.248 sec/batch)\n",
      "2019-10-06 17:51:48.041425: step 4420, loss = 1.01 (52.5 examples/sec; 2.436 sec/batch)\n",
      "2019-10-06 17:52:12.887970: step 4430, loss = 1.00 (51.5 examples/sec; 2.485 sec/batch)\n",
      "2019-10-06 17:52:36.231535: step 4440, loss = 0.85 (54.8 examples/sec; 2.334 sec/batch)\n",
      "2019-10-06 17:53:01.094444: step 4450, loss = 1.27 (51.5 examples/sec; 2.486 sec/batch)\n",
      "2019-10-06 17:53:25.396329: step 4460, loss = 1.34 (52.7 examples/sec; 2.430 sec/batch)\n",
      "2019-10-06 17:53:48.402796: step 4470, loss = 1.06 (55.6 examples/sec; 2.301 sec/batch)\n",
      "2019-10-06 17:54:11.278466: step 4480, loss = 1.24 (56.0 examples/sec; 2.288 sec/batch)\n",
      "2019-10-06 17:54:34.257290: step 4490, loss = 1.07 (55.7 examples/sec; 2.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.422328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:54:57.982834 15980 basic_session_run_hooks.py:692] global_step/sec: 0.422328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:54:58.089682: step 4500, loss = 1.18 (53.7 examples/sec; 2.383 sec/batch)\n",
      "2019-10-06 17:55:21.791254: step 4510, loss = 1.14 (54.0 examples/sec; 2.370 sec/batch)\n",
      "2019-10-06 17:55:45.360216: step 4520, loss = 0.98 (54.3 examples/sec; 2.357 sec/batch)\n",
      "2019-10-06 17:56:08.132308: step 4530, loss = 1.04 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 17:56:30.835586: step 4540, loss = 1.18 (56.4 examples/sec; 2.270 sec/batch)\n",
      "2019-10-06 17:56:53.711403: step 4550, loss = 1.17 (56.0 examples/sec; 2.288 sec/batch)\n",
      "2019-10-06 17:57:16.383764: step 4560, loss = 1.21 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 17:57:39.043452: step 4570, loss = 1.08 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 17:58:01.581173: step 4580, loss = 0.89 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 17:58:24.146819: step 4590, loss = 0.96 (56.7 examples/sec; 2.257 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.437096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:58:46.765323 15980 basic_session_run_hooks.py:692] global_step/sec: 0.437096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:58:46.798236: step 4600, loss = 0.96 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 17:59:09.405768: step 4610, loss = 1.00 (56.6 examples/sec; 2.261 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 4617 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 17:59:22.960515 15980 basic_session_run_hooks.py:606] Saving checkpoints for 4617 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 17:59:32.490707: step 4620, loss = 1.03 (55.4 examples/sec; 2.308 sec/batch)\n",
      "2019-10-06 17:59:55.523104: step 4630, loss = 1.05 (55.6 examples/sec; 2.303 sec/batch)\n",
      "2019-10-06 18:00:18.181878: step 4640, loss = 0.91 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 18:00:40.919065: step 4650, loss = 1.04 (56.3 examples/sec; 2.274 sec/batch)\n",
      "2019-10-06 18:01:03.691159: step 4660, loss = 1.34 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 18:01:26.418372: step 4670, loss = 0.97 (56.3 examples/sec; 2.273 sec/batch)\n",
      "2019-10-06 18:01:49.957416: step 4680, loss = 1.15 (54.4 examples/sec; 2.354 sec/batch)\n",
      "2019-10-06 18:02:12.960890: step 4690, loss = 0.89 (55.6 examples/sec; 2.300 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.436792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:02:35.707053 15980 basic_session_run_hooks.py:692] global_step/sec: 0.436792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:02:35.827275: step 4700, loss = 0.94 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 18:02:58.486664: step 4710, loss = 0.96 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 18:03:21.121448: step 4720, loss = 1.02 (56.6 examples/sec; 2.263 sec/batch)\n",
      "2019-10-06 18:03:43.662774: step 4730, loss = 0.98 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 18:04:06.329150: step 4740, loss = 1.02 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 18:04:28.988804: step 4750, loss = 1.16 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 18:04:51.655183: step 4760, loss = 0.96 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 18:05:14.361451: step 4770, loss = 0.93 (56.4 examples/sec; 2.271 sec/batch)\n",
      "2019-10-06 18:05:37.072706: step 4780, loss = 1.03 (56.4 examples/sec; 2.271 sec/batch)\n",
      "2019-10-06 18:05:59.655759: step 4790, loss = 0.91 (56.7 examples/sec; 2.258 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.441046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:06:22.441815 15980 basic_session_run_hooks.py:692] global_step/sec: 0.441046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:06:22.555470: step 4800, loss = 1.14 (55.9 examples/sec; 2.290 sec/batch)\n",
      "2019-10-06 18:06:45.355200: step 4810, loss = 0.93 (56.1 examples/sec; 2.280 sec/batch)\n",
      "2019-10-06 18:07:07.984675: step 4820, loss = 1.16 (56.6 examples/sec; 2.263 sec/batch)\n",
      "2019-10-06 18:07:30.563769: step 4830, loss = 1.02 (56.7 examples/sec; 2.258 sec/batch)\n",
      "2019-10-06 18:07:53.172302: step 4840, loss = 0.93 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 18:08:15.685087: step 4850, loss = 1.07 (56.9 examples/sec; 2.251 sec/batch)\n",
      "2019-10-06 18:08:38.230441: step 4860, loss = 1.04 (56.8 examples/sec; 2.255 sec/batch)\n",
      "2019-10-06 18:09:00.677597: step 4870, loss = 1.16 (57.0 examples/sec; 2.245 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 4881 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:09:23.210332 15980 basic_session_run_hooks.py:606] Saving checkpoints for 4881 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:09:23.670493: step 4880, loss = 1.07 (55.7 examples/sec; 2.299 sec/batch)\n",
      "2019-10-06 18:09:48.003096: step 4890, loss = 1.13 (52.6 examples/sec; 2.433 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.438065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:10:10.717344 15980 basic_session_run_hooks.py:692] global_step/sec: 0.438065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:10:10.722331: step 4900, loss = 1.21 (56.3 examples/sec; 2.272 sec/batch)\n",
      "2019-10-06 18:10:33.335848: step 4910, loss = 1.01 (56.6 examples/sec; 2.261 sec/batch)\n",
      "2019-10-06 18:10:55.989928: step 4920, loss = 0.94 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 18:11:18.568980: step 4930, loss = 1.33 (56.7 examples/sec; 2.258 sec/batch)\n",
      "2019-10-06 18:11:41.269267: step 4940, loss = 1.19 (56.4 examples/sec; 2.270 sec/batch)\n",
      "2019-10-06 18:12:04.094204: step 4950, loss = 1.12 (56.1 examples/sec; 2.282 sec/batch)\n",
      "2019-10-06 18:12:26.760580: step 4960, loss = 1.01 (56.5 examples/sec; 2.267 sec/batch)\n",
      "2019-10-06 18:12:49.381079: step 4970, loss = 1.08 (56.6 examples/sec; 2.262 sec/batch)\n",
      "2019-10-06 18:13:11.953108: step 4980, loss = 0.85 (56.7 examples/sec; 2.257 sec/batch)\n",
      "2019-10-06 18:13:34.618698: step 4990, loss = 1.19 (56.5 examples/sec; 2.267 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.441387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:13:57.277096 15980 basic_session_run_hooks.py:692] global_step/sec: 0.441387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:13:57.302339: step 5000, loss = 0.99 (56.4 examples/sec; 2.268 sec/batch)\n",
      "2019-10-06 18:14:20.052490: step 5010, loss = 1.09 (56.3 examples/sec; 2.275 sec/batch)\n",
      "2019-10-06 18:14:42.701911: step 5020, loss = 1.07 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 18:15:05.402196: step 5030, loss = 1.21 (56.4 examples/sec; 2.270 sec/batch)\n",
      "2019-10-06 18:15:28.138386: step 5040, loss = 1.00 (56.3 examples/sec; 2.274 sec/batch)\n",
      "2019-10-06 18:15:51.093989: step 5050, loss = 1.09 (55.8 examples/sec; 2.296 sec/batch)\n",
      "2019-10-06 18:16:13.633541: step 5060, loss = 1.11 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 18:16:36.175251: step 5070, loss = 0.94 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 18:16:58.811704: step 5080, loss = 1.17 (56.5 examples/sec; 2.264 sec/batch)\n",
      "2019-10-06 18:17:22.434523: step 5090, loss = 1.03 (54.2 examples/sec; 2.362 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.433646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:17:47.879705 15980 basic_session_run_hooks.py:692] global_step/sec: 0.433646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:17:47.896658: step 5100, loss = 1.12 (50.3 examples/sec; 2.546 sec/batch)\n",
      "2019-10-06 18:18:12.570095: step 5110, loss = 0.97 (51.9 examples/sec; 2.467 sec/batch)\n",
      "2019-10-06 18:18:35.445314: step 5120, loss = 1.00 (56.0 examples/sec; 2.288 sec/batch)\n",
      "2019-10-06 18:18:59.265150: step 5130, loss = 1.11 (53.7 examples/sec; 2.382 sec/batch)\n",
      "2019-10-06 18:19:22.240700: step 5140, loss = 1.00 (55.7 examples/sec; 2.298 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 5142 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:19:24.541546 15980 basic_session_run_hooks.py:606] Saving checkpoints for 5142 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:19:46.709623: step 5150, loss = 0.90 (52.3 examples/sec; 2.447 sec/batch)\n",
      "2019-10-06 18:20:10.477137: step 5160, loss = 0.96 (53.9 examples/sec; 2.377 sec/batch)\n",
      "2019-10-06 18:20:34.434062: step 5170, loss = 1.10 (53.4 examples/sec; 2.396 sec/batch)\n",
      "2019-10-06 18:20:58.794187: step 5180, loss = 0.87 (52.5 examples/sec; 2.436 sec/batch)\n",
      "2019-10-06 18:21:22.561186: step 5190, loss = 1.04 (53.9 examples/sec; 2.377 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.420829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:21:45.504821 15980 basic_session_run_hooks.py:692] global_step/sec: 0.420829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:21:45.619136: step 5200, loss = 1.04 (55.5 examples/sec; 2.306 sec/batch)\n",
      "2019-10-06 18:22:08.509909: step 5210, loss = 0.97 (55.9 examples/sec; 2.289 sec/batch)\n",
      "2019-10-06 18:22:32.020183: step 5220, loss = 0.84 (54.4 examples/sec; 2.351 sec/batch)\n",
      "2019-10-06 18:22:55.374718: step 5230, loss = 0.97 (54.8 examples/sec; 2.335 sec/batch)\n",
      "2019-10-06 18:23:19.170075: step 5240, loss = 1.10 (53.8 examples/sec; 2.380 sec/batch)\n",
      "2019-10-06 18:23:43.060754: step 5250, loss = 1.11 (53.6 examples/sec; 2.389 sec/batch)\n",
      "2019-10-06 18:24:06.227952: step 5260, loss = 1.01 (55.3 examples/sec; 2.317 sec/batch)\n",
      "2019-10-06 18:24:29.752034: step 5270, loss = 0.97 (54.4 examples/sec; 2.352 sec/batch)\n",
      "2019-10-06 18:24:52.624858: step 5280, loss = 1.07 (56.0 examples/sec; 2.287 sec/batch)\n",
      "2019-10-06 18:25:16.467089: step 5290, loss = 0.96 (53.7 examples/sec; 2.384 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.425283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:25:40.642430 15980 basic_session_run_hooks.py:692] global_step/sec: 0.425283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:25:40.697321: step 5300, loss = 0.99 (52.8 examples/sec; 2.423 sec/batch)\n",
      "2019-10-06 18:26:04.659758: step 5310, loss = 1.12 (53.4 examples/sec; 2.396 sec/batch)\n",
      "2019-10-06 18:26:28.826121: step 5320, loss = 1.05 (53.0 examples/sec; 2.417 sec/batch)\n",
      "2019-10-06 18:26:51.714900: step 5330, loss = 0.97 (55.9 examples/sec; 2.289 sec/batch)\n",
      "2019-10-06 18:27:15.501281: step 5340, loss = 1.24 (53.8 examples/sec; 2.379 sec/batch)\n",
      "2019-10-06 18:27:38.993937: step 5350, loss = 0.90 (54.5 examples/sec; 2.349 sec/batch)\n",
      "2019-10-06 18:28:02.894014: step 5360, loss = 0.89 (53.6 examples/sec; 2.390 sec/batch)\n",
      "2019-10-06 18:28:26.205275: step 5370, loss = 1.07 (54.9 examples/sec; 2.331 sec/batch)\n",
      "2019-10-06 18:28:49.848042: step 5380, loss = 0.95 (54.1 examples/sec; 2.364 sec/batch)\n",
      "2019-10-06 18:29:13.616469: step 5390, loss = 1.00 (53.9 examples/sec; 2.377 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 5396 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:29:25.309196 15980 basic_session_run_hooks.py:606] Saving checkpoints for 5396 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.422117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:29:37.543813 15980 basic_session_run_hooks.py:692] global_step/sec: 0.422117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:29:37.547802: step 5400, loss = 1.02 (53.5 examples/sec; 2.393 sec/batch)\n",
      "2019-10-06 18:30:00.378739: step 5410, loss = 1.08 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 18:30:23.369248: step 5420, loss = 1.02 (55.7 examples/sec; 2.299 sec/batch)\n",
      "2019-10-06 18:30:47.103767: step 5430, loss = 1.26 (53.9 examples/sec; 2.373 sec/batch)\n",
      "2019-10-06 18:31:12.239495: step 5440, loss = 1.00 (50.9 examples/sec; 2.514 sec/batch)\n",
      "2019-10-06 18:31:36.149864: step 5450, loss = 0.94 (53.5 examples/sec; 2.391 sec/batch)\n",
      "2019-10-06 18:32:01.571870: step 5460, loss = 0.99 (50.4 examples/sec; 2.542 sec/batch)\n",
      "2019-10-06 18:32:25.643600: step 5470, loss = 0.93 (53.2 examples/sec; 2.407 sec/batch)\n",
      "2019-10-06 18:32:48.843549: step 5480, loss = 1.20 (55.2 examples/sec; 2.320 sec/batch)\n",
      "2019-10-06 18:33:12.599174: step 5490, loss = 0.91 (53.9 examples/sec; 2.376 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.418914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:33:36.256037 15980 basic_session_run_hooks.py:692] global_step/sec: 0.418914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:33:36.300655: step 5500, loss = 1.12 (54.0 examples/sec; 2.370 sec/batch)\n",
      "2019-10-06 18:34:00.246733: step 5510, loss = 1.01 (53.5 examples/sec; 2.395 sec/batch)\n",
      "2019-10-06 18:34:24.256663: step 5520, loss = 0.92 (53.3 examples/sec; 2.401 sec/batch)\n",
      "2019-10-06 18:34:47.628152: step 5530, loss = 0.78 (54.8 examples/sec; 2.337 sec/batch)\n",
      "2019-10-06 18:35:12.976300: step 5540, loss = 0.94 (50.5 examples/sec; 2.535 sec/batch)\n",
      "2019-10-06 18:35:37.164997: step 5550, loss = 1.01 (52.9 examples/sec; 2.419 sec/batch)\n",
      "2019-10-06 18:36:01.766198: step 5560, loss = 0.89 (52.0 examples/sec; 2.460 sec/batch)\n",
      "2019-10-06 18:36:25.438883: step 5570, loss = 0.88 (54.1 examples/sec; 2.367 sec/batch)\n",
      "2019-10-06 18:36:50.069071: step 5580, loss = 0.80 (52.0 examples/sec; 2.463 sec/batch)\n",
      "2019-10-06 18:37:13.926226: step 5590, loss = 0.94 (53.7 examples/sec; 2.386 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.41341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:37:38.147445 15980 basic_session_run_hooks.py:692] global_step/sec: 0.41341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:37:38.177436: step 5600, loss = 1.05 (52.8 examples/sec; 2.425 sec/batch)\n",
      "2019-10-06 18:38:03.265823: step 5610, loss = 0.91 (51.0 examples/sec; 2.509 sec/batch)\n",
      "2019-10-06 18:38:26.639257: step 5620, loss = 0.92 (54.8 examples/sec; 2.337 sec/batch)\n",
      "2019-10-06 18:38:50.256205: step 5630, loss = 1.00 (54.2 examples/sec; 2.362 sec/batch)\n",
      "2019-10-06 18:39:13.755522: step 5640, loss = 0.98 (54.5 examples/sec; 2.350 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 5647 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:39:27.600493 15980 basic_session_run_hooks.py:606] Saving checkpoints for 5647 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:39:37.550523: step 5650, loss = 1.10 (53.8 examples/sec; 2.380 sec/batch)\n",
      "2019-10-06 18:40:00.852201: step 5660, loss = 0.86 (54.9 examples/sec; 2.330 sec/batch)\n",
      "2019-10-06 18:40:24.523888: step 5670, loss = 1.17 (54.1 examples/sec; 2.367 sec/batch)\n",
      "2019-10-06 18:40:47.333600: step 5680, loss = 0.97 (56.1 examples/sec; 2.281 sec/batch)\n",
      "2019-10-06 18:41:11.163863: step 5690, loss = 0.80 (53.7 examples/sec; 2.383 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.422132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:41:35.039006 15980 basic_session_run_hooks.py:692] global_step/sec: 0.422132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:41:35.149527: step 5700, loss = 1.10 (53.4 examples/sec; 2.399 sec/batch)\n",
      "2019-10-06 18:41:58.622737: step 5710, loss = 0.91 (54.5 examples/sec; 2.347 sec/batch)\n",
      "2019-10-06 18:42:22.059053: step 5720, loss = 0.91 (54.6 examples/sec; 2.344 sec/batch)\n",
      "2019-10-06 18:42:45.811526: step 5730, loss = 1.04 (53.9 examples/sec; 2.375 sec/batch)\n",
      "2019-10-06 18:43:09.126451: step 5740, loss = 0.88 (54.9 examples/sec; 2.331 sec/batch)\n",
      "2019-10-06 18:43:32.650661: step 5750, loss = 0.92 (54.4 examples/sec; 2.352 sec/batch)\n",
      "2019-10-06 18:43:55.422718: step 5760, loss = 1.00 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 18:44:18.211767: step 5770, loss = 0.97 (56.2 examples/sec; 2.279 sec/batch)\n",
      "2019-10-06 18:44:40.902079: step 5780, loss = 0.94 (56.4 examples/sec; 2.269 sec/batch)\n",
      "2019-10-06 18:45:03.603362: step 5790, loss = 0.90 (56.4 examples/sec; 2.270 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.432367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:45:26.324841 15980 basic_session_run_hooks.py:692] global_step/sec: 0.432367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:45:26.434479: step 5800, loss = 1.07 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 18:45:49.263418: step 5810, loss = 1.07 (56.1 examples/sec; 2.283 sec/batch)\n",
      "2019-10-06 18:46:11.995619: step 5820, loss = 0.93 (56.3 examples/sec; 2.273 sec/batch)\n",
      "2019-10-06 18:46:34.652533: step 5830, loss = 1.06 (56.5 examples/sec; 2.266 sec/batch)\n",
      "2019-10-06 18:46:57.298961: step 5840, loss = 0.98 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 18:47:19.920458: step 5850, loss = 0.98 (56.6 examples/sec; 2.262 sec/batch)\n",
      "2019-10-06 18:47:42.865090: step 5860, loss = 1.03 (55.8 examples/sec; 2.294 sec/batch)\n",
      "2019-10-06 18:48:05.666108: step 5870, loss = 0.91 (56.1 examples/sec; 2.280 sec/batch)\n",
      "2019-10-06 18:48:28.645646: step 5880, loss = 0.97 (55.7 examples/sec; 2.298 sec/batch)\n",
      "2019-10-06 18:48:52.190672: step 5890, loss = 1.22 (54.4 examples/sec; 2.355 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.436347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:49:15.499331 15980 basic_session_run_hooks.py:692] global_step/sec: 0.436347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:49:15.519857: step 5900, loss = 0.99 (54.9 examples/sec; 2.333 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 5907 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:49:29.573269 15980 basic_session_run_hooks.py:606] Saving checkpoints for 5907 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:49:39.345858: step 5910, loss = 0.79 (53.7 examples/sec; 2.383 sec/batch)\n",
      "2019-10-06 18:50:02.313429: step 5920, loss = 0.92 (55.7 examples/sec; 2.297 sec/batch)\n",
      "2019-10-06 18:50:25.673949: step 5930, loss = 0.86 (54.8 examples/sec; 2.336 sec/batch)\n",
      "2019-10-06 18:50:49.406203: step 5940, loss = 1.07 (53.9 examples/sec; 2.373 sec/batch)\n",
      "2019-10-06 18:51:12.623786: step 5950, loss = 1.06 (55.1 examples/sec; 2.322 sec/batch)\n",
      "2019-10-06 18:51:35.876344: step 5960, loss = 1.06 (55.0 examples/sec; 2.325 sec/batch)\n",
      "2019-10-06 18:51:59.180004: step 5970, loss = 0.94 (54.9 examples/sec; 2.330 sec/batch)\n",
      "2019-10-06 18:52:22.693225: step 5980, loss = 1.01 (54.4 examples/sec; 2.351 sec/batch)\n",
      "2019-10-06 18:52:46.088799: step 5990, loss = 0.87 (54.7 examples/sec; 2.340 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.428651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:53:08.790494 15980 basic_session_run_hooks.py:692] global_step/sec: 0.428651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:53:08.901595: step 6000, loss = 1.15 (56.1 examples/sec; 2.281 sec/batch)\n",
      "2019-10-06 18:53:31.783392: step 6010, loss = 1.05 (55.9 examples/sec; 2.288 sec/batch)\n",
      "2019-10-06 18:53:55.346572: step 6020, loss = 1.16 (54.3 examples/sec; 2.356 sec/batch)\n",
      "2019-10-06 18:54:19.119810: step 6030, loss = 1.10 (53.8 examples/sec; 2.377 sec/batch)\n",
      "2019-10-06 18:54:41.934910: step 6040, loss = 1.04 (56.1 examples/sec; 2.282 sec/batch)\n",
      "2019-10-06 18:55:05.013185: step 6050, loss = 1.06 (55.5 examples/sec; 2.308 sec/batch)\n",
      "2019-10-06 18:55:28.178228: step 6060, loss = 1.07 (55.3 examples/sec; 2.317 sec/batch)\n",
      "2019-10-06 18:55:50.972521: step 6070, loss = 1.00 (56.2 examples/sec; 2.279 sec/batch)\n",
      "2019-10-06 18:56:14.172471: step 6080, loss = 1.02 (55.2 examples/sec; 2.320 sec/batch)\n",
      "2019-10-06 18:56:36.933593: step 6090, loss = 0.81 (56.2 examples/sec; 2.276 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.432264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:57:00.129554 15980 basic_session_run_hooks.py:692] global_step/sec: 0.432264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:57:00.270178: step 6100, loss = 0.87 (54.8 examples/sec; 2.334 sec/batch)\n",
      "2019-10-06 18:57:23.184889: step 6110, loss = 0.92 (55.9 examples/sec; 2.291 sec/batch)\n",
      "2019-10-06 18:57:46.074669: step 6120, loss = 1.03 (55.9 examples/sec; 2.289 sec/batch)\n",
      "2019-10-06 18:58:08.813849: step 6130, loss = 0.89 (56.3 examples/sec; 2.274 sec/batch)\n",
      "2019-10-06 18:58:31.327634: step 6140, loss = 0.89 (56.9 examples/sec; 2.251 sec/batch)\n",
      "2019-10-06 18:58:54.095669: step 6150, loss = 1.09 (56.2 examples/sec; 2.277 sec/batch)\n",
      "2019-10-06 18:59:18.074535: step 6160, loss = 0.90 (53.4 examples/sec; 2.398 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 6166 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:59:29.946781 15980 basic_session_run_hooks.py:606] Saving checkpoints for 6166 into /tmp/cifar10_train\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 18:59:41.554927: step 6170, loss = 0.80 (54.5 examples/sec; 2.348 sec/batch)\n",
      "2019-10-06 19:00:06.007347: step 6180, loss = 1.00 (52.3 examples/sec; 2.445 sec/batch)\n",
      "2019-10-06 19:00:28.624854: step 6190, loss = 0.90 (56.6 examples/sec; 2.262 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.432314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:00:51.443822 15980 basic_session_run_hooks.py:692] global_step/sec: 0.432314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:00:51.446815: step 6200, loss = 0.99 (56.1 examples/sec; 2.282 sec/batch)\n",
      "2019-10-06 19:01:14.086262: step 6210, loss = 1.05 (56.5 examples/sec; 2.264 sec/batch)\n",
      "2019-10-06 19:01:36.629968: step 6220, loss = 0.96 (56.8 examples/sec; 2.254 sec/batch)\n",
      "2019-10-06 19:02:00.023399: step 6230, loss = 0.87 (54.7 examples/sec; 2.339 sec/batch)\n",
      "2019-10-06 19:02:22.677808: step 6240, loss = 0.77 (56.5 examples/sec; 2.265 sec/batch)\n",
      "2019-10-06 19:02:45.389064: step 6250, loss = 0.96 (56.4 examples/sec; 2.271 sec/batch)\n",
      "2019-10-06 19:03:07.960405: step 6260, loss = 0.85 (56.7 examples/sec; 2.257 sec/batch)\n",
      "2019-10-06 19:03:30.558963: step 6270, loss = 0.99 (56.6 examples/sec; 2.260 sec/batch)\n",
      "2019-10-06 19:03:53.253264: step 6280, loss = 0.91 (56.4 examples/sec; 2.269 sec/batch)\n",
      "2019-10-06 19:04:16.307603: step 6290, loss = 0.83 (55.5 examples/sec; 2.305 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 0.438545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:04:39.469655 15980 basic_session_run_hooks.py:692] global_step/sec: 0.438545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:04:39.502916: step 6300, loss = 1.17 (55.2 examples/sec; 2.320 sec/batch)\n",
      "2019-10-06 19:05:02.581187: step 6310, loss = 1.05 (55.5 examples/sec; 2.308 sec/batch)\n",
      "2019-10-06 19:05:25.511858: step 6320, loss = 0.89 (55.8 examples/sec; 2.293 sec/batch)\n",
      "2019-10-06 19:05:48.249044: step 6330, loss = 1.09 (56.3 examples/sec; 2.274 sec/batch)\n",
      "2019-10-06 19:06:12.017473: step 6340, loss = 0.96 (53.9 examples/sec; 2.377 sec/batch)\n",
      "2019-10-06 19:06:36.569721: step 6350, loss = 0.92 (52.1 examples/sec; 2.455 sec/batch)\n",
      "2019-10-06 19:06:59.898570: step 6360, loss = 0.89 (54.9 examples/sec; 2.333 sec/batch)\n",
      "2019-10-06 19:07:25.231813: step 6370, loss = 1.00 (50.5 examples/sec; 2.533 sec/batch)\n",
      "2019-10-06 19:07:49.419122: step 6380, loss = 0.87 (52.9 examples/sec; 2.419 sec/batch)\n",
      "2019-10-06 19:08:13.383028: step 6390, loss = 1.14 (53.4 examples/sec; 2.396 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "%run cifar10_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-06T11:11:43.988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:158: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:151: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:46.823511 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:151: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:153: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:46.825506 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:153: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "I1006 19:11:46.829496 16108 dataset_builder.py:184] Overwrite dataset info from restored data version.\n",
      "I1006 19:11:46.833512 16108 dataset_builder.py:253] Reusing dataset cifar10 (C:\\Users\\lenovouser\\tensorflow_datasets\\cifar10\\1.0.2)\n",
      "I1006 19:11:46.834482 16108 dataset_builder.py:399] Constructing tf.data.Dataset for split test, from C:\\Users\\lenovouser\\tensorflow_datasets\\cifar10\\1.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_input.py:75: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.574794 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_input.py:75: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.601761 16108 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_input.py:45: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.616710 16108 deprecation.py:323] From D:\\jupyter\\cifar10_input.py:45: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_input.py:48: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.638650 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_input.py:48: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.643636 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.644608 16108 deprecation.py:506] From D:\\jupyter\\cifar10.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:102: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.648601 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:102: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:85: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.667545 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:85: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:86: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.671535 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:86: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:190: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.694474 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:190: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:47.867012 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:136: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:48.037914 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:136: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:139: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:48.053871 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:139: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:141: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:48.057873 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:141: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:71: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:48.204572 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:71: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:49.331001 16108 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:11:49.347913 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\jupyter\\cifar10_eval.py:88: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 19:11:49.440658 16108 deprecation_wrapper.py:119] From D:\\jupyter\\cifar10_eval.py:88: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:12:02.941388: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:12:09.783734 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:12:21.352581: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:12:28.024852 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:12:39.898096: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:12:47.010521 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:12:59.237454: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:13:06.038663 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:13:18.927302: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:13:26.015112 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:13:39.143886: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:13:45.998784 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:13:59.138795: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:14:06.329766 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:14:19.376067: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:14:26.530783 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:14:39.845482: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:14:47.066264 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:15:00.327324: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:15:07.447179 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:15:20.993471: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:15:28.141071 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-06 19:15:42.020890: precision @ 1 = 0.791\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 19:15:49.253497 16108 saver.py:1280] Restoring parameters from /tmp/cifar10_train\\model.ckpt-6166\n"
     ]
    }
   ],
   "source": [
    "%run cifar10_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRN（局部响应归一化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T05:24:04.346554Z",
     "start_time": "2019-10-06T05:23:21.516590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3  4]\n",
      "   [ 5  6  7  8]]\n",
      "\n",
      "  [[ 9 10 11 12]\n",
      "   [13 14 15 16]]]\n",
      "\n",
      "\n",
      " [[[17 18 19 20]\n",
      "   [21 22 23 24]]\n",
      "\n",
      "  [[25 26 27 28]\n",
      "   [29 30 31 32]]]]\n",
      "#############\n",
      "[[[[0.07142857 0.06666667 0.10000001 0.13793103]\n",
      "   [0.04545454 0.03448276 0.04022989 0.05369128]]\n",
      "\n",
      "  [[0.02980132 0.02242153 0.02466368 0.03287672]\n",
      "   [0.0220339  0.01654846 0.0177305  0.02363368]]]\n",
      "\n",
      "\n",
      " [[[0.0174538  0.01310044 0.01382824 0.01843318]\n",
      "   [0.01444292 0.01083744 0.01133005 0.01510384]]\n",
      "\n",
      "  [[0.01231527 0.00923952 0.00959488 0.01279123]\n",
      "   [0.01073279 0.00805153 0.00831991 0.01109185]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = np.array([i for i in range(1,33)]).reshape([2,2,2,4])\n",
    "y = tf.nn.lrn(input=x,depth_radius=2,bias=0,alpha=1,beta=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(x)\n",
    "    print('#############')\n",
    "    print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "429.28px",
    "left": "288px",
    "top": "88.4px",
    "width": "245.76px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
